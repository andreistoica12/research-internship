{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all variables in the current environment (if you have already run some cells) - clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Replace the download directory of the NLTK tokenizer files with your preferred directory (I chose the root directory of the Research Internship project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not hasattr(sys, 'argv'):\n",
    "    sys.argv = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/andreistoica12/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/andreistoica12/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "from sentistrength import PySentiStr\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "# Load the punkt tokenizer data from the local directory\n",
    "nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_path = '/home/andreistoica12/research-internship'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1 subfolder to store important graphs. If it already existed (from previous runnings of the project), delete the folder and its contents and create an empty folder to store the current graphs, relevant to the current state of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_path = os.path.join(rootdir_path, 'graphs')\n",
    "if os.path.exists(graphs_path):\n",
    "   shutil.rmtree(graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxxy_graphs_path = os.path.join(graphs_path, 'covaxxy')\n",
    "if os.path.exists(covaxxy_graphs_path):\n",
    "   shutil.rmtree(covaxxy_graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(covaxxy_graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxxy_longitudinal_analysis_graphs = os.path.join(covaxxy_graphs_path, 'longitudinal-analysis')\n",
    "if os.path.exists(covaxxy_longitudinal_analysis_graphs):\n",
    "   shutil.rmtree(covaxxy_longitudinal_analysis_graphs, ignore_errors=False, onerror=None)\n",
    "os.makedirs(covaxxy_longitudinal_analysis_graphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where we store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = rootdir_path + '/data/covaxxy-csv-complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength = rootdir_path + '/SentiStrength'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength_jar = path_to_sentistrength + '/SentiStrengthCom.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength_language_folder = path_to_sentistrength + '/LanguageFolder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = rootdir_path + '/files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_replies_opinion_changes = files_path + '/replies_opinion_changes.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_stopwords = files_path + '/stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stop_words(path_to_stopwords):\n",
    "    stop_words = set()\n",
    "    with open(path_to_stopwords, 'r') as f:\n",
    "        for line in f:\n",
    "            word = line.strip()  # remove whitespace and newline characters\n",
    "            stop_words.add(word)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = custom_stop_words(path_to_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I can use the predefined list of stopwords provided by NLTK, but it's for general purpose\n",
    "# # and the results when computing the sentiment are worse than expected, e.g. it considers\n",
    "# # words, such as \"not\" and \"all\" to be stopwords in contexts where they are actually important.\n",
    "# # So, I will use a custom stopwords list.\n",
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function remove_emoji() that takes a text string as input and uses a regular expression to match all Unicode characters that are classified as emojis. The regular expression includes different ranges of Unicode characters that represent different types of emojis, such as emoticons, symbols, and flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stop_words):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove the stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stop_words):    \n",
    "    # 1. Lowercase all words in the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Replace the new line character with empty string\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # 3. Remove words starting with '@' - tags (most common noise in replies)\n",
    "    text = re.sub(r'@\\w+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 4. Remove words starting with 'http' - hyperlinks\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 5. Remove punctuation from the text using regular expressions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 6. Remove contractions, such as you're => you are\n",
    "    contractions.fix(text)\n",
    "\n",
    "    # 7. Remove emojis\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    # 8. Remove stopwords in English\n",
    "    text = remove_stopwords(text, stop_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_date(path):\n",
    "        # Read the CSV file into a pandas dataframe\n",
    "        df_from_file = pd.read_csv(path, index_col= False)\n",
    "            \n",
    "        # Convert the \"created_at\" column to a pandas datetime object\n",
    "        df_from_file['created_at'] = pd.to_datetime(df_from_file['created_at'])\n",
    "\n",
    "        # Get all unique timestamp values from the \"created_at\" column\n",
    "        unique_dates = df_from_file['created_at'].dt.date.unique()\n",
    "\n",
    "        # Create a dictionary where the keys are the unique timestamp values\n",
    "        # and the values are dataframes that correspond to each unique timestamp value\n",
    "        days = {}\n",
    "        for date in unique_dates:\n",
    "            # Extract the rows that have the current timestamp value\n",
    "            mask = df_from_file['created_at'].dt.date == date\n",
    "            filtered_df = df_from_file[mask]\n",
    "            # Store the resulting subset of rows as a dataframe in the dictionary\n",
    "            days[date] = filtered_df\n",
    "        \n",
    "        return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_days(data_path):\n",
    "    # In order to read the data from the files, I need the paths of the files to be passed on to the read_csv() function. \n",
    "    # The order of the days in the file paths needs to be consistent with the order of the dates in the keys\n",
    "    file_paths = [ os.path.join(data_path, file) for file in os.listdir(data_path) ]\n",
    "\n",
    "    # Set the number of processes to run in parallel\n",
    "    num_processes = multiprocessing.cpu_count() * 2\n",
    "    # Create a pool of workers to execute the filter_df_by_date function\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        # Use the pool to execute the filter_df_by_date function on each file in parallel\n",
    "        results = pool.map(filter_df_by_date, file_paths)\n",
    "\n",
    "    days = dict()\n",
    "    for result in results:\n",
    "        days = {k: pd.concat([days.get(k, pd.DataFrame()), result.get(k, pd.DataFrame())]) for k in set(days) | set(result)}\n",
    "\n",
    "    # Dictionary comprehension to format datetime object keys to strings - useful for ease of accessing\n",
    "    days = {datetime_key.strftime('%d-%m-%Y'): df for datetime_key, df in days.items()}\n",
    "\n",
    "    # Iterate over all the keys in the dictionary\n",
    "    for key in days.keys():\n",
    "        days[key].sort_values('created_at', inplace=True)\n",
    "        # Drop the \"id\" column from the dataframe corresponding to the key\n",
    "        days[key].drop('id', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = create_days(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_days(days):\n",
    "    # Here, I merged all data (from all available days) into a single dataframe (they have the same structure).\n",
    "    # I did that because some replies to a tweet posted today can come some days after, so we need to take care\n",
    "    # of the dataset as a whole.\n",
    "\n",
    "    \n",
    "    # Convert string keys to datetime objects and sort them\n",
    "    sorted_keys = sorted([datetime.strptime(k, '%d-%m-%Y') for k in days.keys()])\n",
    "\n",
    "    # Convert datetime objects back to string keys with format '%d-%m-%Y'\n",
    "    sorted_key_strings = [k.strftime('%d-%m-%Y') for k in sorted_keys]\n",
    "\n",
    "    # concatenate the dataframes and reset the index\n",
    "    merged_days = pd.concat([days[key] for key in sorted_key_strings], ignore_index=True)\n",
    "\n",
    "    # Convert string column to datetime\n",
    "    merged_days['created_at'] = pd.to_datetime(merged_days['created_at'])\n",
    "\n",
    "    return merged_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_days = create_merged_days(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>credible</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>urls</th>\n",
       "      <th>name</th>\n",
       "      <th>username</th>\n",
       "      <th>verified</th>\n",
       "      <th>location</th>\n",
       "      <th>...</th>\n",
       "      <th>retweet_author_id</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>retweeted_screen_name</th>\n",
       "      <th>user_mentions_id</th>\n",
       "      <th>user_mentions_screen_name</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_tweet_id</th>\n",
       "      <th>in_reply_to_username</th>\n",
       "      <th>reference_type</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-24 18:00:10+00:00</td>\n",
       "      <td>1364636249852502018</td>\n",
       "      <td>1</td>\n",
       "      <td>107501328</td>\n",
       "      <td>RT @Maricopahealth: At one of our community po...</td>\n",
       "      <td>#</td>\n",
       "      <td>2-1-1 Arizona</td>\n",
       "      <td>211arizona</td>\n",
       "      <td>False</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>...</td>\n",
       "      <td>29816986</td>\n",
       "      <td>1364632754042802176</td>\n",
       "      <td>Maricopahealth</td>\n",
       "      <td>29816986</td>\n",
       "      <td>Maricopahealth</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1364632754042802176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-24 18:00:18+00:00</td>\n",
       "      <td>1364636282664574978</td>\n",
       "      <td>1</td>\n",
       "      <td>26761523</td>\n",
       "      <td>Ready for DAY 2 of State of the Valley? Join u...</td>\n",
       "      <td>jointventure.org,twitter.com,</td>\n",
       "      <td>Joint Venture SV</td>\n",
       "      <td>JointVentureSVN</td>\n",
       "      <td>False</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-24 18:00:30+00:00</td>\n",
       "      <td>1364636333596008449</td>\n",
       "      <td>1</td>\n",
       "      <td>1234926105234034689</td>\n",
       "      <td>RT @SteveStaeger: When #COVID19Colorado is ove...</td>\n",
       "      <td>#</td>\n",
       "      <td>Colorado Coronavirus Updates</td>\n",
       "      <td>COVIDinColorado</td>\n",
       "      <td>False</td>\n",
       "      <td>Denver, Colorado</td>\n",
       "      <td>...</td>\n",
       "      <td>182037688</td>\n",
       "      <td>1364293582157307906</td>\n",
       "      <td>SteveStaeger</td>\n",
       "      <td>182037688</td>\n",
       "      <td>SteveStaeger</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1364293582157307906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-24 18:03:16+00:00</td>\n",
       "      <td>1364637028948709377</td>\n",
       "      <td>1</td>\n",
       "      <td>1329106574082641920</td>\n",
       "      <td>#SD37: Starting next week, @OCHealth will star...</td>\n",
       "      <td>bit.ly,www.ocregister.com,</td>\n",
       "      <td>Senator Dave Min</td>\n",
       "      <td>SenDaveMin</td>\n",
       "      <td>True</td>\n",
       "      <td>Irvine, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>36069538</td>\n",
       "      <td>ochealth</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-24 18:03:35+00:00</td>\n",
       "      <td>1364637110951583746</td>\n",
       "      <td>1</td>\n",
       "      <td>1363750425459970048</td>\n",
       "      <td>RT @jatinde45666597: Vaccination has been star...</td>\n",
       "      <td>#</td>\n",
       "      <td>Reena Sharma</td>\n",
       "      <td>write2reena</td>\n",
       "      <td>False</td>\n",
       "      <td>Auckland, New Zealand</td>\n",
       "      <td>...</td>\n",
       "      <td>1295748297529884673</td>\n",
       "      <td>1364087633538859008</td>\n",
       "      <td>jatinde45666597</td>\n",
       "      <td>1295748297529884673</td>\n",
       "      <td>jatinde45666597</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1364087633538859008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123691</th>\n",
       "      <td>2021-03-10 23:59:52+00:00</td>\n",
       "      <td>1369800203939745796</td>\n",
       "      <td>1</td>\n",
       "      <td>434360613</td>\n",
       "      <td>RT @Philo: The boys of #SouthPark are at it ag...</td>\n",
       "      <td>#</td>\n",
       "      <td>ami_</td>\n",
       "      <td>ami_tvdfan</td>\n",
       "      <td>False</td>\n",
       "      <td>#</td>\n",
       "      <td>...</td>\n",
       "      <td>81766872</td>\n",
       "      <td>1369799981763162113</td>\n",
       "      <td>philoTV</td>\n",
       "      <td>23827692</td>\n",
       "      <td>ComedyCentral</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369799981763162113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123692</th>\n",
       "      <td>2021-03-10 23:59:52+00:00</td>\n",
       "      <td>1369800204094963712</td>\n",
       "      <td>1</td>\n",
       "      <td>3083078947</td>\n",
       "      <td>RT @ericswalwell: The #AmericanRescuePlan puts...</td>\n",
       "      <td>#</td>\n",
       "      <td>Thomas Albrecht üá∫üá∏‚òÆÔ∏è</td>\n",
       "      <td>TomAlb88</td>\n",
       "      <td>False</td>\n",
       "      <td>#</td>\n",
       "      <td>...</td>\n",
       "      <td>377609596</td>\n",
       "      <td>1369727803768201218</td>\n",
       "      <td>ericswalwell</td>\n",
       "      <td>377609596</td>\n",
       "      <td>ericswalwell</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369727803768201218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123693</th>\n",
       "      <td>2021-03-10 23:59:53+00:00</td>\n",
       "      <td>1369800204761899011</td>\n",
       "      <td>1</td>\n",
       "      <td>29801287</td>\n",
       "      <td>RT @TheDweck: Wow, vision boards work</td>\n",
       "      <td>#</td>\n",
       "      <td>Fauxnly Fans</td>\n",
       "      <td>thenickkontz</td>\n",
       "      <td>False</td>\n",
       "      <td>√úT: 43.508306,-96.779489</td>\n",
       "      <td>...</td>\n",
       "      <td>98247788</td>\n",
       "      <td>1369742802590990336</td>\n",
       "      <td>TheDweck</td>\n",
       "      <td>98247788</td>\n",
       "      <td>TheDweck</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369742802590990336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123694</th>\n",
       "      <td>2021-03-10 23:59:53+00:00</td>\n",
       "      <td>1369800205445521409</td>\n",
       "      <td>1</td>\n",
       "      <td>1095155084</td>\n",
       "      <td>just saw some lady on the news say she‚Äôs not g...</td>\n",
       "      <td>#</td>\n",
       "      <td>cristal‚ú®</td>\n",
       "      <td>cristal_guz</td>\n",
       "      <td>False</td>\n",
       "      <td>Raleigh |22|</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123695</th>\n",
       "      <td>2021-03-10 23:59:53+00:00</td>\n",
       "      <td>1369800205592363018</td>\n",
       "      <td>1</td>\n",
       "      <td>3538956135</td>\n",
       "      <td>RT @Doc_Wolverine: \"Gee Doc, why are you pisse...</td>\n",
       "      <td>#</td>\n",
       "      <td>Jackaxed</td>\n",
       "      <td>Jackaxed</td>\n",
       "      <td>False</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>898321581444911108</td>\n",
       "      <td>1369771222481985543</td>\n",
       "      <td>Doc_Wolverine</td>\n",
       "      <td>898321581444911108</td>\n",
       "      <td>Doc_Wolverine</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369771222481985543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5123696 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at             tweet_id  credible  \\\n",
       "0       2021-02-24 18:00:10+00:00  1364636249852502018         1   \n",
       "1       2021-02-24 18:00:18+00:00  1364636282664574978         1   \n",
       "2       2021-02-24 18:00:30+00:00  1364636333596008449         1   \n",
       "3       2021-02-24 18:03:16+00:00  1364637028948709377         1   \n",
       "4       2021-02-24 18:03:35+00:00  1364637110951583746         1   \n",
       "...                           ...                  ...       ...   \n",
       "5123691 2021-03-10 23:59:52+00:00  1369800203939745796         1   \n",
       "5123692 2021-03-10 23:59:52+00:00  1369800204094963712         1   \n",
       "5123693 2021-03-10 23:59:53+00:00  1369800204761899011         1   \n",
       "5123694 2021-03-10 23:59:53+00:00  1369800205445521409         1   \n",
       "5123695 2021-03-10 23:59:53+00:00  1369800205592363018         1   \n",
       "\n",
       "                   author_id  \\\n",
       "0                  107501328   \n",
       "1                   26761523   \n",
       "2        1234926105234034689   \n",
       "3        1329106574082641920   \n",
       "4        1363750425459970048   \n",
       "...                      ...   \n",
       "5123691            434360613   \n",
       "5123692           3083078947   \n",
       "5123693             29801287   \n",
       "5123694           1095155084   \n",
       "5123695           3538956135   \n",
       "\n",
       "                                                      text  \\\n",
       "0        RT @Maricopahealth: At one of our community po...   \n",
       "1        Ready for DAY 2 of State of the Valley? Join u...   \n",
       "2        RT @SteveStaeger: When #COVID19Colorado is ove...   \n",
       "3        #SD37: Starting next week, @OCHealth will star...   \n",
       "4        RT @jatinde45666597: Vaccination has been star...   \n",
       "...                                                    ...   \n",
       "5123691  RT @Philo: The boys of #SouthPark are at it ag...   \n",
       "5123692  RT @ericswalwell: The #AmericanRescuePlan puts...   \n",
       "5123693              RT @TheDweck: Wow, vision boards work   \n",
       "5123694  just saw some lady on the news say she‚Äôs not g...   \n",
       "5123695  RT @Doc_Wolverine: \"Gee Doc, why are you pisse...   \n",
       "\n",
       "                                  urls                          name  \\\n",
       "0                                    #                 2-1-1 Arizona   \n",
       "1        jointventure.org,twitter.com,              Joint Venture SV   \n",
       "2                                    #  Colorado Coronavirus Updates   \n",
       "3           bit.ly,www.ocregister.com,              Senator Dave Min   \n",
       "4                                    #                  Reena Sharma   \n",
       "...                                ...                           ...   \n",
       "5123691                              #                          ami_   \n",
       "5123692                              #          Thomas Albrecht üá∫üá∏‚òÆÔ∏è   \n",
       "5123693                              #                  Fauxnly Fans   \n",
       "5123694                              #                      cristal‚ú®   \n",
       "5123695                              #                      Jackaxed   \n",
       "\n",
       "                username  verified                  location  ...  \\\n",
       "0             211arizona     False                   Arizona  ...   \n",
       "1        JointVentureSVN     False              San Jose, CA  ...   \n",
       "2        COVIDinColorado     False          Denver, Colorado  ...   \n",
       "3             SenDaveMin      True                Irvine, CA  ...   \n",
       "4            write2reena     False     Auckland, New Zealand  ...   \n",
       "...                  ...       ...                       ...  ...   \n",
       "5123691       ami_tvdfan     False                         #  ...   \n",
       "5123692         TomAlb88     False                         #  ...   \n",
       "5123693     thenickkontz     False  √úT: 43.508306,-96.779489  ...   \n",
       "5123694      cristal_guz     False              Raleigh |22|  ...   \n",
       "5123695         Jackaxed     False             United States  ...   \n",
       "\n",
       "           retweet_author_id           retweet_id  retweeted_screen_name  \\\n",
       "0                   29816986  1364632754042802176         Maricopahealth   \n",
       "1                          #                    #                      #   \n",
       "2                  182037688  1364293582157307906           SteveStaeger   \n",
       "3                          #                    #                      #   \n",
       "4        1295748297529884673  1364087633538859008        jatinde45666597   \n",
       "...                      ...                  ...                    ...   \n",
       "5123691             81766872  1369799981763162113                philoTV   \n",
       "5123692            377609596  1369727803768201218           ericswalwell   \n",
       "5123693             98247788  1369742802590990336               TheDweck   \n",
       "5123694                    #                    #                      #   \n",
       "5123695   898321581444911108  1369771222481985543          Doc_Wolverine   \n",
       "\n",
       "            user_mentions_id  user_mentions_screen_name  in_reply_to_user_id  \\\n",
       "0                   29816986             Maricopahealth                    #   \n",
       "1                          #                          #                    #   \n",
       "2                  182037688               SteveStaeger                    #   \n",
       "3                   36069538                   ochealth                    #   \n",
       "4        1295748297529884673            jatinde45666597                    #   \n",
       "...                      ...                        ...                  ...   \n",
       "5123691             23827692              ComedyCentral                    #   \n",
       "5123692            377609596               ericswalwell                    #   \n",
       "5123693             98247788                   TheDweck                    #   \n",
       "5123694                    #                          #                    #   \n",
       "5123695   898321581444911108              Doc_Wolverine                    #   \n",
       "\n",
       "         in_reply_to_tweet_id in_reply_to_username reference_type  \\\n",
       "0                           #                    #      retweeted   \n",
       "1                           #                    #              #   \n",
       "2                           #                    #      retweeted   \n",
       "3                           #                    #              #   \n",
       "4                           #                    #      retweeted   \n",
       "...                       ...                  ...            ...   \n",
       "5123691                     #                    #      retweeted   \n",
       "5123692                     #                    #      retweeted   \n",
       "5123693                     #                    #      retweeted   \n",
       "5123694                     #                    #              #   \n",
       "5123695                     #                    #      retweeted   \n",
       "\n",
       "                reference_id  \n",
       "0        1364632754042802176  \n",
       "1                          #  \n",
       "2        1364293582157307906  \n",
       "3                          #  \n",
       "4        1364087633538859008  \n",
       "...                      ...  \n",
       "5123691  1369799981763162113  \n",
       "5123692  1369727803768201218  \n",
       "5123693  1369742802590990336  \n",
       "5123694                    #  \n",
       "5123695  1369771222481985543  \n",
       "\n",
       "[5123696 rows x 27 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_days"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = merged_days[merged_days['reference_type'] == 'quoted'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>credible</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>urls</th>\n",
       "      <th>name</th>\n",
       "      <th>username</th>\n",
       "      <th>verified</th>\n",
       "      <th>location</th>\n",
       "      <th>...</th>\n",
       "      <th>retweet_author_id</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>retweeted_screen_name</th>\n",
       "      <th>user_mentions_id</th>\n",
       "      <th>user_mentions_screen_name</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_tweet_id</th>\n",
       "      <th>in_reply_to_username</th>\n",
       "      <th>reference_type</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-02-24 18:05:18+00:00</td>\n",
       "      <td>1364637540330872838</td>\n",
       "      <td>1</td>\n",
       "      <td>1069764336279683072</td>\n",
       "      <td>SO many people don't realize how many inmates ...</td>\n",
       "      <td>twitter.com,</td>\n",
       "      <td>Chern like üßà</td>\n",
       "      <td>VisualsByChern</td>\n",
       "      <td>False</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>quoted</td>\n",
       "      <td>1364636471387246592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021-02-24 18:27:32+00:00</td>\n",
       "      <td>1364643136199299072</td>\n",
       "      <td>1</td>\n",
       "      <td>43545377</td>\n",
       "      <td>Just a reminder, we will have plenty of vaccin...</td>\n",
       "      <td>twitter.com,</td>\n",
       "      <td>Ann Bibby</td>\n",
       "      <td>anniegirl1138</td>\n",
       "      <td>False</td>\n",
       "      <td>Tea Time</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>quoted</td>\n",
       "      <td>1364583293295943681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021-02-24 18:42:16+00:00</td>\n",
       "      <td>1364646846434435074</td>\n",
       "      <td>1</td>\n",
       "      <td>139836595</td>\n",
       "      <td>Funny how he supports the Genocide of the Abor...</td>\n",
       "      <td>twitter.com,</td>\n",
       "      <td>mart chris</td>\n",
       "      <td>marchris52</td>\n",
       "      <td>False</td>\n",
       "      <td>BC üá®üá¶</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>quoted</td>\n",
       "      <td>1364351771209043970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2021-02-24 19:05:30+00:00</td>\n",
       "      <td>1364652691750748166</td>\n",
       "      <td>1</td>\n",
       "      <td>754541231372275712</td>\n",
       "      <td>Gd what a genius idea I WILL be taking ur advi...</td>\n",
       "      <td>twitter.com,</td>\n",
       "      <td>Aidan Chase</td>\n",
       "      <td>HPEveryoneLives</td>\n",
       "      <td>False</td>\n",
       "      <td>#</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>427037072</td>\n",
       "      <td>waywardskyeyes</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>quoted</td>\n",
       "      <td>1364289562734960641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2021-02-24 19:23:33+00:00</td>\n",
       "      <td>1364657233888309249</td>\n",
       "      <td>1</td>\n",
       "      <td>24785956</td>\n",
       "      <td>Think this will be a policy change, soon... ht...</td>\n",
       "      <td>twitter.com,</td>\n",
       "      <td>Mark I Williams M.D.</td>\n",
       "      <td>CameraGuyBakoCA</td>\n",
       "      <td>False</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>quoted</td>\n",
       "      <td>1364654324387872772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123640</th>\n",
       "      <td>2021-03-10 23:59:44+00:00</td>\n",
       "      <td>1369800167386357762</td>\n",
       "      <td>1</td>\n",
       "      <td>1244589997186887683</td>\n",
       "      <td>I Trump had acted responsibly and listened to ...</td>\n",
       "      <td>twitter.com,</td>\n",
       "      <td>Rand Paul's Neighbor - parody it's good to laugh</td>\n",
       "      <td>jbrown11871</td>\n",
       "      <td>False</td>\n",
       "      <td>Earth</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>quoted</td>\n",
       "      <td>1369793211833741312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123641</th>\n",
       "      <td>2021-03-10 23:59:44+00:00</td>\n",
       "      <td>1369800166698582025</td>\n",
       "      <td>1</td>\n",
       "      <td>1100444916</td>\n",
       "      <td>That‚Äôs that chalkasain in you https://t.co/WBO...</td>\n",
       "      <td>twitter.com,</td>\n",
       "      <td>Amiri ambassador</td>\n",
       "      <td>treyfive_</td>\n",
       "      <td>False</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>quoted</td>\n",
       "      <td>1369797672660594695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123661</th>\n",
       "      <td>2021-03-10 23:59:47+00:00</td>\n",
       "      <td>1369800181881962497</td>\n",
       "      <td>1</td>\n",
       "      <td>2422329920</td>\n",
       "      <td>Remember this in the Coming Dark Days; \\nIn 19...</td>\n",
       "      <td>twitter.com,</td>\n",
       "      <td>‚ùå‚ùå McFixit1 ‚ùå‚ùå</td>\n",
       "      <td>navstadt</td>\n",
       "      <td>False</td>\n",
       "      <td>Panther Burn MS</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>quoted</td>\n",
       "      <td>1369644619747778562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123665</th>\n",
       "      <td>2021-03-10 23:59:48+00:00</td>\n",
       "      <td>1369800185786765314</td>\n",
       "      <td>1</td>\n",
       "      <td>3730426754</td>\n",
       "      <td>üí•BREAKING NEWSüí•\\n\\n#PostcardsToVoters #Resist ...</td>\n",
       "      <td>twitter.com,</td>\n",
       "      <td>Tall Miracle üá∫üá¶‚òÆÔ∏èüó≥üì¨</td>\n",
       "      <td>miracleguppy</td>\n",
       "      <td>False</td>\n",
       "      <td>Boise, ID USA</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>quoted</td>\n",
       "      <td>1369774083118891009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123683</th>\n",
       "      <td>2021-03-10 23:59:51+00:00</td>\n",
       "      <td>1369800198571110406</td>\n",
       "      <td>1</td>\n",
       "      <td>184571032</td>\n",
       "      <td>Job openings at this Kroger: https://t.co/xDIx...</td>\n",
       "      <td>twitter.com,</td>\n",
       "      <td>#87, can you do somethin for me‚Ä¶</td>\n",
       "      <td>Droa26</td>\n",
       "      <td>False</td>\n",
       "      <td>#</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>quoted</td>\n",
       "      <td>1369775813529321478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342898 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at             tweet_id  credible  \\\n",
       "8       2021-02-24 18:05:18+00:00  1364637540330872838         1   \n",
       "28      2021-02-24 18:27:32+00:00  1364643136199299072         1   \n",
       "45      2021-02-24 18:42:16+00:00  1364646846434435074         1   \n",
       "69      2021-02-24 19:05:30+00:00  1364652691750748166         1   \n",
       "84      2021-02-24 19:23:33+00:00  1364657233888309249         1   \n",
       "...                           ...                  ...       ...   \n",
       "5123640 2021-03-10 23:59:44+00:00  1369800167386357762         1   \n",
       "5123641 2021-03-10 23:59:44+00:00  1369800166698582025         1   \n",
       "5123661 2021-03-10 23:59:47+00:00  1369800181881962497         1   \n",
       "5123665 2021-03-10 23:59:48+00:00  1369800185786765314         1   \n",
       "5123683 2021-03-10 23:59:51+00:00  1369800198571110406         1   \n",
       "\n",
       "                   author_id  \\\n",
       "8        1069764336279683072   \n",
       "28                  43545377   \n",
       "45                 139836595   \n",
       "69        754541231372275712   \n",
       "84                  24785956   \n",
       "...                      ...   \n",
       "5123640  1244589997186887683   \n",
       "5123641           1100444916   \n",
       "5123661           2422329920   \n",
       "5123665           3730426754   \n",
       "5123683            184571032   \n",
       "\n",
       "                                                      text          urls  \\\n",
       "8        SO many people don't realize how many inmates ...  twitter.com,   \n",
       "28       Just a reminder, we will have plenty of vaccin...  twitter.com,   \n",
       "45       Funny how he supports the Genocide of the Abor...  twitter.com,   \n",
       "69       Gd what a genius idea I WILL be taking ur advi...  twitter.com,   \n",
       "84       Think this will be a policy change, soon... ht...  twitter.com,   \n",
       "...                                                    ...           ...   \n",
       "5123640  I Trump had acted responsibly and listened to ...  twitter.com,   \n",
       "5123641  That‚Äôs that chalkasain in you https://t.co/WBO...  twitter.com,   \n",
       "5123661  Remember this in the Coming Dark Days; \\nIn 19...  twitter.com,   \n",
       "5123665  üí•BREAKING NEWSüí•\\n\\n#PostcardsToVoters #Resist ...  twitter.com,   \n",
       "5123683  Job openings at this Kroger: https://t.co/xDIx...  twitter.com,   \n",
       "\n",
       "                                                     name         username  \\\n",
       "8                                            Chern like üßà   VisualsByChern   \n",
       "28                                              Ann Bibby    anniegirl1138   \n",
       "45                                             mart chris       marchris52   \n",
       "69                                            Aidan Chase  HPEveryoneLives   \n",
       "84                                   Mark I Williams M.D.  CameraGuyBakoCA   \n",
       "...                                                   ...              ...   \n",
       "5123640  Rand Paul's Neighbor - parody it's good to laugh      jbrown11871   \n",
       "5123641                                  Amiri ambassador        treyfive_   \n",
       "5123661                                    ‚ùå‚ùå McFixit1 ‚ùå‚ùå         navstadt   \n",
       "5123665                               Tall Miracle üá∫üá¶‚òÆÔ∏èüó≥üì¨     miracleguppy   \n",
       "5123683                  #87, can you do somethin for me‚Ä¶           Droa26   \n",
       "\n",
       "         verified         location  ...  retweet_author_id  retweet_id  \\\n",
       "8           False  California, USA  ...                  #           #   \n",
       "28          False         Tea Time  ...                  #           #   \n",
       "45          False           BC üá®üá¶   ...                  #           #   \n",
       "69          False                #  ...                  #           #   \n",
       "84          False  Bakersfield, CA  ...                  #           #   \n",
       "...           ...              ...  ...                ...         ...   \n",
       "5123640     False            Earth  ...                  #           #   \n",
       "5123641     False      Switzerland  ...                  #           #   \n",
       "5123661     False  Panther Burn MS  ...                  #           #   \n",
       "5123665     False    Boise, ID USA  ...                  #           #   \n",
       "5123683     False                #  ...                  #           #   \n",
       "\n",
       "         retweeted_screen_name  user_mentions_id  user_mentions_screen_name  \\\n",
       "8                            #                 #                          #   \n",
       "28                           #                 #                          #   \n",
       "45                           #                 #                          #   \n",
       "69                           #         427037072             waywardskyeyes   \n",
       "84                           #                 #                          #   \n",
       "...                        ...               ...                        ...   \n",
       "5123640                      #                 #                          #   \n",
       "5123641                      #                 #                          #   \n",
       "5123661                      #                 #                          #   \n",
       "5123665                      #                 #                          #   \n",
       "5123683                      #                 #                          #   \n",
       "\n",
       "         in_reply_to_user_id  in_reply_to_tweet_id in_reply_to_username  \\\n",
       "8                          #                     #                    #   \n",
       "28                         #                     #                    #   \n",
       "45                         #                     #                    #   \n",
       "69                         #                     #                    #   \n",
       "84                         #                     #                    #   \n",
       "...                      ...                   ...                  ...   \n",
       "5123640                    #                     #                    #   \n",
       "5123641                    #                     #                    #   \n",
       "5123661                    #                     #                    #   \n",
       "5123665                    #                     #                    #   \n",
       "5123683                    #                     #                    #   \n",
       "\n",
       "        reference_type         reference_id  \n",
       "8               quoted  1364636471387246592  \n",
       "28              quoted  1364583293295943681  \n",
       "45              quoted  1364351771209043970  \n",
       "69              quoted  1364289562734960641  \n",
       "84              quoted  1364654324387872772  \n",
       "...                ...                  ...  \n",
       "5123640         quoted  1369793211833741312  \n",
       "5123641         quoted  1369797672660594695  \n",
       "5123661         quoted  1369644619747778562  \n",
       "5123665         quoted  1369774083118891009  \n",
       "5123683         quoted  1369775813529321478  \n",
       "\n",
       "[342898 rows x 27 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Think this will be a policy change, soon... https://t.co/Pffvij3RKf'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes.loc[84, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1364657233888309249"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes.loc[84, 'tweet_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1364654324387872772'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes.loc[84, 'reference_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_quote_texts = quotes.head(200).loc[:, 'text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file for writing (use 'a' instead of 'w' to append)\n",
    "with open(files_path + '/first_200_quotes.txt', 'w') as file:\n",
    "    # Loop through the list_of_quote_texts and write each text to a new line in the file\n",
    "    for text in list_of_quote_texts:\n",
    "        file.write(text + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_types = ['replied_to', 'quoted', 'retweeted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_reactions(merged_days, reaction_types):\n",
    "    reactions = merged_days[merged_days['reference_type'].isin(reaction_types)]\n",
    "    multiple_reactions = reactions[reactions.duplicated(subset=['author_id', 'reference_id'], keep=False)].copy()\n",
    "    multiple_reactions['reference_id'] = multiple_reactions['reference_id'].astype(int)\n",
    "\n",
    "    # group the rows by the two columns\n",
    "    grouped_df = multiple_reactions.groupby(['author_id', 'reference_id'])\n",
    "    groups_of_reactions = grouped_df.groups\n",
    "\n",
    "    return groups_of_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_of_reactions = group_reactions(merged_days, reaction_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34501"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups_of_reactions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath(path_to_sentistrength_jar)\n",
    "senti.setSentiStrengthLanguageFolderPath(path_to_sentistrength_language_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiments(rows_indices, reactions, stop_words):\n",
    "    texts = [ clean_text(reactions.loc[index, 'text'], stop_words) \n",
    "             if reactions.loc[index, 'reference_type'] != 'retweeted' else 'extremely fabulous'\n",
    "             for index in rows_indices ]\n",
    "    \n",
    "    sentiments = senti.getSentiment(texts, score='scale')\n",
    "\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_change(rows_indices, reactions, stop_words):\n",
    "    \"\"\"Function to detect whether an opinion change occured within a group of reactions (replies/quotes/retweets).\n",
    "\n",
    "    Args:\n",
    "        rows_indices (pandas.core.indexes.numeric.Int64Index): list of indices in the original dataframe\n",
    "                                                               where an opinion change has been detected\n",
    "                                                               (e.g. Int64Index([1848965, 1850146, 1850687], dtype='int64'))\n",
    "\n",
    "    Returns:\n",
    "        bool: boolean value which confirms or denies the existence of an opinion change within a group\n",
    "    \"\"\" \n",
    "    sentiments = compute_sentiments(rows_indices, reactions, stop_words)\n",
    "    sentiments = np.array(sentiments)\n",
    "\n",
    "    positive = np.any(sentiments > 0)\n",
    "    negative = np.any(sentiments < 0)\n",
    "\n",
    "    return positive and negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATION OF TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(merged_days):\n",
    "    test_replies = merged_days[merged_days['reference_type'] == 'replied_to']\n",
    "    multiple_test_replies = test_replies[test_replies.duplicated(subset=['author_id', 'reference_id'], keep=False)].copy()\n",
    "    multiple_test_replies['reference_id'] = multiple_test_replies['reference_id'].astype(int)\n",
    "    small_multiple_test_replies = multiple_test_replies.head(2000).copy()\n",
    "\n",
    "    # group the rows by the two columns\n",
    "    grouped_df = small_multiple_test_replies.groupby(['author_id', 'reference_id'])\n",
    "    # grouped_df = multiple_test_replies.groupby(['author_id','reference_id'])\n",
    "    groups_of_test_replies = grouped_df.groups\n",
    "\n",
    "    return groups_of_test_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_test_data = create_test_data(merged_days)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE:\n",
    "\n",
    "THERE IS NO NEED TO RUN THE COMMENTED CELLS BETWEEN THE LINES BELOW!\n",
    "It took more than 15 minutes when I ran the creation of the opinion_changes dictionary on the whole replies dataset...\n",
    "\n",
    "I saved the resulting dictionary into a JSON file, which can be found in the root directory of the project. This can be imported into a dictionary with ease (code can be found in the next parts of the notebook)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEQUENTIAL COMPUTATION - OPINION CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counter = 0\n",
    "progress = 0.001\n",
    "\n",
    "def print_progress(groups_of_reactions):\n",
    "    \"\"\"Function that prints the progress of the computation of the opinion_changes dictionary,\n",
    "    as it takes a lot of time for large datasets.\n",
    "\n",
    "    Args:\n",
    "        groups_of_replies (dict): dictionary of replies grouped by some columns\n",
    "    \"\"\"    \n",
    "    global group_counter\n",
    "    global progress\n",
    "    group_counter += 1\n",
    "\n",
    "    if ((group_counter / len(groups_of_reactions)) >= progress):\n",
    "        print(f\"Progress: {group_counter} / {len(groups_of_reactions)} groups of reactions processed.\")\n",
    "        progress += 0.001\n",
    "    if group_counter == len(groups_of_reactions):\n",
    "        print(\"All groups have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opinion_changes(groups_of_reactions, reactions, progress_printing, stop_words):\n",
    "    \"\"\"Function to create the data structure associated with the groups (pairs of user id-s who interacted through replies)\n",
    "    where an opinio change occured, i.e. when, between their interactions (e.g. one's replies to the other's original post),\n",
    "    there have been both positive and negative opinions.\n",
    "\n",
    "    Args:\n",
    "        groups_of_replies (dict): dictionary of replies grouped by some columns\n",
    "        progress_printing (bool): boolean value indicating whether the user wishes to print the progress of the groups processed or not\n",
    "                                  (this can be useful to track when processing large datasets - they usually take a lot of time)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary where the keys represent the groups where opinion changes occured (as tuples) and the values are\n",
    "              lists of the sentiments associated to the interactions within each group\n",
    "    \"\"\"    \n",
    "    if progress_printing == True:\n",
    "        opinion_changes = {}\n",
    "        for group, rows_indices in groups_of_reactions.items():\n",
    "            print_progress(groups_of_reactions)\n",
    "            if opinion_change(rows_indices, reactions, stop_words) == True:\n",
    "                opinion_changes[group] = compute_sentiments(rows_indices, reactions, stop_words)\n",
    "    else:\n",
    "        print(\"Gradual progress will not be printed.\")\n",
    "        print(\"If you wish to see it, change the value of the progress_printing input parameter to True.\")\n",
    "        opinion_changes = { group: compute_sentiments(rows_indices, reactions, stop_words) for group, rows_indices in groups_of_reactions.items() \n",
    "                        if opinion_change(rows_indices, reactions, stop_words) == True }\n",
    "    \n",
    "    return opinion_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_printing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_changes = create_opinion_changes(groups_test_data, merged_days, progress_printing, stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARALLEL COMPUTATION - OPINION CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_values(rows_indexes):\n",
    "    global merged_days\n",
    "    global stop_words\n",
    "    processed_values = []\n",
    "    if opinion_change(rows_indexes, merged_days, stop_words):\n",
    "        texts = [ clean_text(merged_days.loc[index, 'text'], stop_words) for index in rows_indexes ]\n",
    "        \n",
    "        processed_values = senti.getSentiment(texts, score='scale')\n",
    "\n",
    "    return processed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dict_chunk(input_dict):\n",
    "    # Process a chunk of the input dictionary\n",
    "    processed_dict = {}\n",
    "    counter = 0\n",
    "    progress = 0.0001\n",
    "    \n",
    "    for key, index_list in input_dict.items():\n",
    "        processed_values = process_values(index_list)\n",
    "        if processed_values:  # only add non-empty lists to the dictionary\n",
    "            processed_dict[key] = processed_values\n",
    "\n",
    "        counter += 1\n",
    "        if ((counter / len(input_dict)) >= progress):\n",
    "            print(f\"Processed {counter} / {len(input_dict)} entries\\n\")\n",
    "            progress += 0.0001\n",
    "        if counter == len(input_dict):\n",
    "            print(f\"Process has finished processing all {len(input_dict)} entries.\")\n",
    "\n",
    "\n",
    "    return processed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dict_in_parallel(input_dict, num_processes=None):\n",
    "    # Default to using all available CPU cores\n",
    "    if num_processes is None:\n",
    "        num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # Split the input dictionary into smaller chunks for parallel processing\n",
    "    chunk_size = len(input_dict) // num_processes\n",
    "    input_chunks = [dict(list(input_dict.items())[i:i + chunk_size]) for i in range(0, len(input_dict), chunk_size)]\n",
    "\n",
    "    # Process the input chunks in parallel using a pool of worker processes\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        processed_dicts = pool.map(process_dict_chunk, input_chunks)\n",
    "\n",
    "    # Merge the processed dictionaries from each input chunk\n",
    "    processed_dict = {}\n",
    "    for d in processed_dicts:\n",
    "        processed_dict.update(d)\n",
    "\n",
    "    return processed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_changes_parallel = process_dict_in_parallel(groups_of_reactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7482"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opinion_changes_parallel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE DICTIONARY TO JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_to_opinion_changes(reaction_types):\n",
    "    type = \"_\".join(reaction_types)\n",
    "    path = files_path + f\"/{type}_opinion_changes.json\"\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_opinion_changes_to_JSON(opinion_changes, reaction_types):\n",
    "    \"\"\"Function to save the dictionary of opinion changes to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        opinion_changes (dict): dictionary with opinion changes\n",
    "        path (str): path where you wish to save the JSON file\n",
    "    \"\"\"    \n",
    "    path = create_path_to_opinion_changes(reaction_types)\n",
    "\n",
    "    # create a new dictionary with string keys\n",
    "    opinion_changes_for_JSON_file = {str(key): value for key, value in opinion_changes.items() }\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(opinion_changes_for_JSON_file, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_opinion_changes_to_JSON(opinion_changes_parallel, reaction_types)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DICTIONARY FROM JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opinion_changes(path_to_opinion_changes):\n",
    "    \"\"\"Function that generates a dictionary based on a JSON file which contains the opinion changes within the replies of the dataset.\n",
    "\n",
    "    Args:\n",
    "        path_to_replies_opinion_changes (str): path to the JSON file associated with the opinion changes within the replies\n",
    "                                               (e.g. /your/path/to/research-internship)\n",
    "\n",
    "    Returns:\n",
    "        dict: the original dictionary containing opinion changes from replies\n",
    "    \"\"\"    \n",
    "    with open(path_to_opinion_changes) as f:\n",
    "        # Load the JSON data into a Python dictionary\n",
    "        opinion_changes_from_file = json.load(f)\n",
    "        # Create a new dictionary with tuple keys\n",
    "        original_opinion_changes = {}\n",
    "        for key in opinion_changes_from_file:\n",
    "            # Convert the string key to a tuple\n",
    "            new_key = eval(key)\n",
    "            # Add the key-value pair to the new dictionary\n",
    "            original_opinion_changes[new_key] = opinion_changes_from_file[key]\n",
    "            \n",
    "    return original_opinion_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_opinion_changes = create_path_to_opinion_changes(reaction_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_changes = load_opinion_changes(path_to_opinion_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7482"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opinion_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_changes == opinion_changes_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of opinion changes out of the interactions where one user replied multiple times to a source tweet:\n",
      "21.7%.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of opinion changes out of the interactions where one user replied multiple times to a source tweet:\")\n",
    "print(f\"{round(len(opinion_changes) / len(groups_of_reactions) * 100, 1)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biggest_opinion_change(opinion_changes):\n",
    "    \"\"\"Function that returns the group (pair of user id-s) which interacted more than once in the context of a single source tweet,\n",
    "    i.e. one user posted more than one reply to the same source tweet, where the user who reacted had the most drastic opinion change,\n",
    "    based on the previously computed sentiments of the text.\n",
    "\n",
    "    Args:\n",
    "        opinion_changes (dict): dictionary with opinion changes\n",
    "\n",
    "    Returns:\n",
    "        tuple: pair of user id-s where the biggest opinion change occured\n",
    "        str: type of change that occured, e.g. one user tends to agree with the source tweet after some time, \n",
    "             when initially he disagreed or vice-versa\n",
    "    \"\"\"    \n",
    "    change_type = 'negative'\n",
    "    biggest_change = 0\n",
    "    target_group = tuple()\n",
    "    for group, sentiments in opinion_changes.items():\n",
    "        change = max(biggest_change, max(sentiments) - min(sentiments))\n",
    "        if change > biggest_change:\n",
    "            biggest_change = change\n",
    "            target_group = group\n",
    "    \n",
    "    min_sentiment_index = opinion_changes[target_group].index(min(opinion_changes[target_group]))\n",
    "    max_sentiment_index = opinion_changes[target_group].index(max(opinion_changes[target_group]))\n",
    "    change_type = 'positive' if min_sentiment_index < max_sentiment_index else change_type\n",
    "\n",
    "    return target_group, change_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_group, change_type = biggest_opinion_change(opinion_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118788479, 1367613364923424769)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replies_with_biggest_opinion_change(multiple_replies, target_group):\n",
    "    \"\"\"Function that queries the multiple_replies dataset and returns a list of the actual texts that the pair of users posted.\n",
    "     The user id-s of these users are passed on as input parameters (the target group).\n",
    "\n",
    "    Args:\n",
    "        replies (pandas Dataframe): the dataframe with the replies\n",
    "        target_group (tuple): pair of user id-s whose posts had the biggest opinion change\n",
    "\n",
    "    Returns:\n",
    "        list: list of texts posted by the 2 users\n",
    "    \"\"\"    \n",
    "    condition1 = multiple_replies['author_id'] == target_group[0]\n",
    "    condition2 = multiple_replies['in_reply_to_tweet_id'] == target_group[1]\n",
    "\n",
    "    return multiple_replies[condition1 & condition2].loc[:, 'text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_biggest_change = replies_with_biggest_opinion_change(multiple_replies, target_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@KATIEDOLL1201 @daulan @SandySue1958 I've been taking vaccines since I was very young..  I got the vax -my SIL was SORRY she didn't.  My mom and her sis just got both covid shots -NP.  Mom's 90, her sis it is 94.  Grandma was born in 1899, saw the 1st pandemic.. She loved vaccines. She lived to 94, maybe that's why.\",\n",
       " \"@KATIEDOLL1201 @daulan @SandySue1958 Sorry, they're being so hard on you, but the 'majority' of people who get Shingles say it is very painful, and the 'majority' of people who have the vaccine do not have an issue.  That is by the numbers.  And right now there is soo much misinformation being spread- It's just sad.\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies_biggest_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_change_type(opinion_changes, group):\n",
    "    \"\"\"Function to detect what type of opinion change occured in the case of a group (pair of user ids-s) which interacted\n",
    "    through replies\n",
    "\n",
    "    Args:\n",
    "        opinion_changes (dict): dictionary with opinion changes\n",
    "        group (tuple): pair of user id-s that interacted through replies and the respondent changed his viewpoint w.r.t. a source tweet\n",
    "\n",
    "    Returns:\n",
    "        str: either 'positive' (if the respondent now agrees after initially disagreeing) or 'negative'\n",
    "    \"\"\"    \n",
    "    min_sentiment_index = opinion_changes[group].index(min(opinion_changes[group]))\n",
    "    max_sentiment_index = opinion_changes[group].index(max(opinion_changes[group]))\n",
    "    \n",
    "    change_type = 'negative'\n",
    "    change_type = 'positive' if min_sentiment_index < max_sentiment_index else change_type\n",
    "\n",
    "    return change_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask indicating what type of opinion change each group has\n",
    "mask = {group: opinion_change_type(opinion_changes, group) for group in opinion_changes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_count_in_dict(dict, value_to_count):\n",
    "    \"\"\"Function to count the occurences of a certain value in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        dict (dict): dictionary where we need to count the occurences of a value\n",
    "        value_to_count (any): value to be counted\n",
    "\n",
    "    Returns:\n",
    "        int: number of occurences of value_to_count\n",
    "    \"\"\"    \n",
    "    # Create a reverse dictionary that maps values to their frequencies\n",
    "    reverse_dict = defaultdict(int)\n",
    "    for value in dict.values():\n",
    "        reverse_dict[value] += 1\n",
    "\n",
    "    # Count the occurrences of the specific value\n",
    "    count = reverse_dict.get(value_to_count, 0)\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive opinion changes out of:\n",
      "- the interactions where one user replied multiple times to a source tweet and an opinion change was detected => 32.2%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of positive opinion changes out of:\")\n",
    "print(f\"- the interactions where one user replied multiple times to a source tweet and an opinion change was detected => {round(value_count_in_dict(mask, 'positive') / len(mask) * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of negative opinion changes out of:\n",
      "- the interactions where one user replied multiple times to a source tweet and an opinion change was detected => 67.8%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of negative opinion changes out of:\")\n",
    "print(f\"- the interactions where one user replied multiple times to a source tweet and an opinion change was detected => {round(value_count_in_dict(mask, 'negative') / len(mask) * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the distribution of the tweets per hour, I will parse the \"created_at\" column, extract the hour property and create a separate column in each dataframe. I will place it next to the \"created_at\" column in order to be easily verifiable. Data originates frmo the Twitter API, so it comes in a standard ISO 8601 format, which can be easily parsed using the parser module from the dateutil package.\n",
    "\n",
    "Note: the cell below runs for approximately 2m30' on my machine (~25-30 seconds for each file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, day in days.items():\n",
    "#     if 'hour' not in day.columns:\n",
    "#         day.insert(1, 'hour', day['created_at'].apply(lambda date: parser.parse(date).hour))\n",
    "#         print(f\"New 'hour' column inserted in the {key} dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, day in days.items():\n",
    "#     if 'hour' not in day.columns:\n",
    "#         hours = []\n",
    "#         for time in day.loc[:,\"created_at\"]:\n",
    "#             hour = parser.parse(time).hour\n",
    "#             hours.append(hour)\n",
    "#         day.insert(1, \"hour\", hours, True)\n",
    "#         print(key + \" - added 'hour' column\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final distribution is made up of the sum of all individual days' distributions. I save a figure in the graphs/ folder for each day, as well as an overall distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_distribution = pd.Series(0, index=days['1-3-2021'].loc[:,'hour'].sort_values(ascending=True).unique())\n",
    "# for key, day in days.items():\n",
    "#     hour_column_ascending = day.loc[:,\"hour\"].sort_values(ascending=True)\n",
    "#     distribution = hour_column_ascending.value_counts()[hour_column_ascending.unique()]\n",
    "#     final_distribution = final_distribution.add(distribution)\n",
    "#     axes = distribution.plot(kind='bar')\n",
    "#     figure_path = f\"{covaxxy_longitudinal_analysis_graphs}/{key}_distribution.png\"\n",
    "#     axes.figure.savefig(figure_path)\n",
    "#     plt.close()\n",
    "# axes = final_distribution.plot(kind='bar')\n",
    "# figure_path = f\"{covaxxy_longitudinal_analysis_graphs}/overall_distribution.png\"\n",
    "# axes.figure.savefig(figure_path)\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243101c165aceacaf115b49fc146265224cf91574f24df3021157e0d2dabdb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
