{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all variables in the current environment (if you have already run some cells) - clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Replace the download directory of the NLTK tokenizer files with your preferred directory (I chose the root directory of the Research Internship project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/andreistoica12/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/andreistoica12/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from sentistrength import PySentiStr\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "# Load the punkt tokenizer data from the local directory\n",
    "nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.core.getipython import get_ipython\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import textwrap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_path = '/home/andreistoica12/research-internship'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where the dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = rootdir_path + '/data/covaxxy-csv-complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = rootdir_path + '/files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_changes_path = files_path + '/opinion-changes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_path = rootdir_path + '/graphs'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1 subfolder within the graphs/ folder to store important graphs for the covaxxy dataset. If it already existed (from previous runnings of the project), delete the folder and its contents and create an empty folder to store the current graphs, relevant to the current state of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxxy_graphs_path = os.path.join(graphs_path, 'covaxxy')\n",
    "if os.path.exists(covaxxy_graphs_path):\n",
    "   shutil.rmtree(covaxxy_graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(covaxxy_graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxxy_deltas_OC_graphs_path = os.path.join(covaxxy_graphs_path, 'deltas-OC')\n",
    "if os.path.exists(covaxxy_deltas_OC_graphs_path):\n",
    "   shutil.rmtree(covaxxy_deltas_OC_graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(covaxxy_deltas_OC_graphs_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subfolders specific to the different types of analyses performed in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covaxxy_longitudinal_analysis_graphs_path = os.path.join(covaxxy_graphs_path, 'longitudinal-analysis')\n",
    "# if os.path.exists(covaxxy_longitudinal_analysis_graphs_path):\n",
    "#    shutil.rmtree(covaxxy_longitudinal_analysis_graphs_path, ignore_errors=False, onerror=None)\n",
    "# os.makedirs(covaxxy_longitudinal_analysis_graphs_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where the SentiStrength library is stored.\n",
    "\n",
    "NOTE: Due to their policy, the Java version of the library (the one I am using) is only free for academic use. Therefore, I could not make it publicly available. If you wish to use the free library (for academic purposes), I will gladly redirect you to the author at M.Thelwall@wlv.ac.uk . \n",
    "\n",
    "More information is available at: http://sentistrength.wlv.ac.uk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength = rootdir_path + '/SentiStrength'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the Java executable file of SentiStrength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength_jar = path_to_sentistrength + '/SentiStrengthCom.jar'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the language folder, which is used along with the .jar file to compute sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength_language_folder = path_to_sentistrength + '/LanguageFolder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the general nature of the NLTK built-in stop words list, most words could actually have an impact in the computation of sentiment scores if removed from the texts (e.g. \"all\" or \"not\"), thus I decided against using this pre-defined list. Instead I created a custom list of stop words, which can be found at the following relative location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_stopwords = files_path + '/stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stop_words(path_to_stopwords):\n",
    "    \"\"\"Function to read a .txt file containing (custom) stop words and return a set of these stop words.\n",
    "\n",
    "    Args:\n",
    "        path_to_stopwords (str): path to the.txt file containing stop words (e.g. /your/path/to/files/stop_words.txt)\n",
    "\n",
    "    Returns:\n",
    "        set: set of stop words\n",
    "    \"\"\"    \n",
    "    stop_words = set()\n",
    "    with open(path_to_stopwords, 'r') as f:\n",
    "        for line in f:\n",
    "            word = line.strip()  # remove whitespace and newline characters\n",
    "            stop_words.add(word)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = custom_stop_words(path_to_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    \"\"\"Function that takes a text string as input and uses a regular expression pattern to match all Unicode characters\n",
    "    that are classified as emojis. The regular expression includes different ranges of Unicode characters \n",
    "    that represent different types of emojis, such as emoticons, symbols, and flags.\n",
    "\n",
    "    Args:\n",
    "        text (str): text string to remove emokis from\n",
    "\n",
    "    Returns:\n",
    "        str: text string with all emojis removed\n",
    "    \"\"\"    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stop_words):\n",
    "    \"\"\"Function that removes stop words from a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): text string\n",
    "        stop_words (set): set of stop words\n",
    "\n",
    "    Returns:\n",
    "        str: text string without stop words\n",
    "    \"\"\"    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove the stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stop_words):\n",
    "    \"\"\"Function to clean the raw text, e.g. from a tweet. Performs the following steps:\n",
    "    1. Lowercase all the words in the text\n",
    "    2. Replace all new line characters with a white space\n",
    "    3. Remove tags\n",
    "    4. Remove URLs\n",
    "    5. Remove punctuations\n",
    "    6. Convert contractions to their full forms\n",
    "    7. Remove emojis (emoticons, symbols, flags, etc.)\n",
    "    8. Remove stopwords\n",
    "\n",
    "\n",
    "    Args:\n",
    "        text (str): text string to be cleaned before passing it to the sentiment analysis model\n",
    "        stop_words (set): set of stop words to be removed from the text\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned text string\n",
    "    \"\"\"        \n",
    "    # 1. Lowercase all words in the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Replace the new line character with empty string\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # 3. Remove words starting with '@' - tags (most common noise in replies)\n",
    "    text = re.sub(r'@\\w+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 4. Remove words starting with 'http' - hyperlinks\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 5. Remove punctuation from the text using regular expressions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 6. Remove contractions, such as you're => you are\n",
    "    contractions.fix(text)\n",
    "\n",
    "    # 7. Remove emojis\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    # 8. Remove stopwords in English\n",
    "    text = remove_stopwords(text, stop_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_date(path):\n",
    "    \"\"\"Function to filter a dataframe by date, given the path to the csv file. \n",
    "    Returns a dictionary with the dates as keys and the values being the rows where the value of the 'created_at' column\n",
    "    corresponds to the key.\n",
    "\n",
    "    Args:\n",
    "        path (str): path to the csv file (e.g. /your/path/to/tweet_ids--2021-03-01.csv)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with the dates as keys and the values being the rows where the value of the 'created_at' column corresponds\n",
    "              to the key\n",
    "    \"\"\"    \n",
    "    # Read the CSV file into a pandas dataframe\n",
    "    df_from_file = pd.read_csv(path, index_col= False)\n",
    "        \n",
    "    # Convert the \"created_at\" column to a pandas datetime object\n",
    "    df_from_file['created_at'] = pd.to_datetime(df_from_file['created_at'])\n",
    "\n",
    "    # Get all unique timestamp values from the \"created_at\" column\n",
    "    unique_dates = df_from_file['created_at'].dt.date.unique()\n",
    "\n",
    "    # Create a dictionary where the keys are the unique timestamp values\n",
    "    # and the values are dataframes that correspond to each unique timestamp value\n",
    "    days = {}\n",
    "    for date in unique_dates:\n",
    "        # Extract the rows that have the current timestamp value\n",
    "        mask = df_from_file['created_at'].dt.date == date\n",
    "        filtered_df = df_from_file[mask]\n",
    "        # Store the resulting subset of rows as a dataframe in the dictionary\n",
    "        days[date] = filtered_df\n",
    "    \n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_days(data_path):\n",
    "    \"\"\"Function to create the merged days dictionary, performed using parallel computation.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): path to the folder where the .csv files are stored\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary containing the merged/concatenated days dictionary, based on all available files\n",
    "    \"\"\"    \n",
    "    # In order to read the data from the files, I need the paths of the files to be passed on to the read_csv() function. \n",
    "    file_paths = [ os.path.join(data_path, file) for file in os.listdir(data_path) ]\n",
    "\n",
    "    # Set the number of processes to run in parallel\n",
    "    num_processes = multiprocessing.cpu_count() * 2\n",
    "    # Create a pool of workers to execute the filter_df_by_date function\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        # Use the pool to execute the filter_df_by_date function on each file in parallel\n",
    "        results = pool.map(filter_df_by_date, file_paths)\n",
    "\n",
    "    days = dict()\n",
    "    # Loop that merges all separate days dictionaries obtained after running the parallel computation\n",
    "    # into one final dictionary, associated with all available data.\n",
    "    for result in results:\n",
    "        days = {k: pd.concat([days.get(k, pd.DataFrame()), result.get(k, pd.DataFrame())]) for k in set(days) | set(result)}\n",
    "\n",
    "    # Dictionary comprehension to format datetime object keys to strings - useful for ease of accessing further down the line.\n",
    "    days = {datetime_key.strftime('%d-%m-%Y'): df for datetime_key, df in days.items()}\n",
    "\n",
    "    # Iterate over all the keys in the dictionary\n",
    "    for key in days.keys():\n",
    "        days[key].sort_values('created_at', inplace=True)\n",
    "        # Drop the \"id\" column from the dataframe corresponding to each key\n",
    "        days[key].drop('id', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = create_days(data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXACT DAYS IN OUR DATASET:\n",
    "\n",
    "Note: I double checked which days were actually used in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dates_datetimes = sorted([ datetime.strptime(date_string, '%d-%m-%Y') for date_string in days.keys() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24-02-2021',\n",
       " '25-02-2021',\n",
       " '26-02-2021',\n",
       " '27-02-2021',\n",
       " '28-02-2021',\n",
       " '01-03-2021',\n",
       " '02-03-2021',\n",
       " '03-03-2021',\n",
       " '04-03-2021',\n",
       " '05-03-2021',\n",
       " '06-03-2021',\n",
       " '07-03-2021',\n",
       " '08-03-2021',\n",
       " '09-03-2021',\n",
       " '10-03-2021']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ date_object.strftime('%d-%m-%Y') for date_object in sorted_dates_datetimes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_days(days):\n",
    "    \"\"\"Function to merge all available days in the days dictionary into a single pandas Dataframe.\n",
    "    The resulting DataFrame will be sorted by date (each value in the input days dictionary has sorted values and\n",
    "    I make sure I sort the keys in ascending order as well, when iterating through the dictionary and creating the\n",
    "    dataframe).\n",
    "\n",
    "    Args:\n",
    "        days (dict): dictionary where the keys are the date strings and the values are dataframes containing the\n",
    "                     rows of each day in the whole dataset\n",
    "\n",
    "    Returns:\n",
    "        pandas.core.frame.DataFrame: dataframe containing the merged days, sorted in ascending order by date\n",
    "    \"\"\"    \n",
    "    # Here, I merged all data (from all available days) into a single dataframe (they have the same structure).\n",
    "    # I did that because some replies to a tweet posted today can come some days after, so we need to take care\n",
    "    # of the dataset as a whole.\n",
    "\n",
    "    \n",
    "    # Convert string keys to datetime objects and sort them\n",
    "    sorted_keys = sorted([datetime.strptime(k, '%d-%m-%Y') for k in days.keys()])\n",
    "\n",
    "    # Convert datetime objects back to string keys with format '%d-%m-%Y'\n",
    "    sorted_key_strings = [k.strftime('%d-%m-%Y') for k in sorted_keys]\n",
    "\n",
    "    # concatenate the dataframes and reset the index\n",
    "    merged_days = pd.concat([days[key] for key in sorted_key_strings], ignore_index=True)\n",
    "\n",
    "\n",
    "    def string_to_int(reference_id):\n",
    "        try:\n",
    "            return int(reference_id)\n",
    "        except ValueError:\n",
    "            return reference_id\n",
    "\n",
    "    # change the data type of the 'reference_id' column from string to int - where possible (the '#' values remain the same)\n",
    "    merged_days['reference_id'] = merged_days['reference_id'].apply(string_to_int)\n",
    "\n",
    "    return merged_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_days = create_merged_days(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>credible</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>urls</th>\n",
       "      <th>name</th>\n",
       "      <th>username</th>\n",
       "      <th>verified</th>\n",
       "      <th>location</th>\n",
       "      <th>...</th>\n",
       "      <th>retweet_author_id</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>retweeted_screen_name</th>\n",
       "      <th>user_mentions_id</th>\n",
       "      <th>user_mentions_screen_name</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_tweet_id</th>\n",
       "      <th>in_reply_to_username</th>\n",
       "      <th>reference_type</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-24 18:00:10+00:00</td>\n",
       "      <td>1364636249852502018</td>\n",
       "      <td>1</td>\n",
       "      <td>107501328</td>\n",
       "      <td>RT @Maricopahealth: At one of our community po...</td>\n",
       "      <td>#</td>\n",
       "      <td>2-1-1 Arizona</td>\n",
       "      <td>211arizona</td>\n",
       "      <td>False</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>...</td>\n",
       "      <td>29816986</td>\n",
       "      <td>1364632754042802176</td>\n",
       "      <td>Maricopahealth</td>\n",
       "      <td>29816986</td>\n",
       "      <td>Maricopahealth</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1364632754042802176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-24 18:00:18+00:00</td>\n",
       "      <td>1364636282664574978</td>\n",
       "      <td>1</td>\n",
       "      <td>26761523</td>\n",
       "      <td>Ready for DAY 2 of State of the Valley? Join u...</td>\n",
       "      <td>jointventure.org,twitter.com,</td>\n",
       "      <td>Joint Venture SV</td>\n",
       "      <td>JointVentureSVN</td>\n",
       "      <td>False</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-24 18:00:30+00:00</td>\n",
       "      <td>1364636333596008449</td>\n",
       "      <td>1</td>\n",
       "      <td>1234926105234034689</td>\n",
       "      <td>RT @SteveStaeger: When #COVID19Colorado is ove...</td>\n",
       "      <td>#</td>\n",
       "      <td>Colorado Coronavirus Updates</td>\n",
       "      <td>COVIDinColorado</td>\n",
       "      <td>False</td>\n",
       "      <td>Denver, Colorado</td>\n",
       "      <td>...</td>\n",
       "      <td>182037688</td>\n",
       "      <td>1364293582157307906</td>\n",
       "      <td>SteveStaeger</td>\n",
       "      <td>182037688</td>\n",
       "      <td>SteveStaeger</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1364293582157307906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-24 18:03:16+00:00</td>\n",
       "      <td>1364637028948709377</td>\n",
       "      <td>1</td>\n",
       "      <td>1329106574082641920</td>\n",
       "      <td>#SD37: Starting next week, @OCHealth will star...</td>\n",
       "      <td>bit.ly,www.ocregister.com,</td>\n",
       "      <td>Senator Dave Min</td>\n",
       "      <td>SenDaveMin</td>\n",
       "      <td>True</td>\n",
       "      <td>Irvine, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>36069538</td>\n",
       "      <td>ochealth</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-24 18:03:35+00:00</td>\n",
       "      <td>1364637110951583746</td>\n",
       "      <td>1</td>\n",
       "      <td>1363750425459970048</td>\n",
       "      <td>RT @jatinde45666597: Vaccination has been star...</td>\n",
       "      <td>#</td>\n",
       "      <td>Reena Sharma</td>\n",
       "      <td>write2reena</td>\n",
       "      <td>False</td>\n",
       "      <td>Auckland, New Zealand</td>\n",
       "      <td>...</td>\n",
       "      <td>1295748297529884673</td>\n",
       "      <td>1364087633538859008</td>\n",
       "      <td>jatinde45666597</td>\n",
       "      <td>1295748297529884673</td>\n",
       "      <td>jatinde45666597</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1364087633538859008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123691</th>\n",
       "      <td>2021-03-10 23:59:52+00:00</td>\n",
       "      <td>1369800203939745796</td>\n",
       "      <td>1</td>\n",
       "      <td>434360613</td>\n",
       "      <td>RT @Philo: The boys of #SouthPark are at it ag...</td>\n",
       "      <td>#</td>\n",
       "      <td>ami_</td>\n",
       "      <td>ami_tvdfan</td>\n",
       "      <td>False</td>\n",
       "      <td>#</td>\n",
       "      <td>...</td>\n",
       "      <td>81766872</td>\n",
       "      <td>1369799981763162113</td>\n",
       "      <td>philoTV</td>\n",
       "      <td>23827692</td>\n",
       "      <td>ComedyCentral</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369799981763162113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123692</th>\n",
       "      <td>2021-03-10 23:59:52+00:00</td>\n",
       "      <td>1369800204094963712</td>\n",
       "      <td>1</td>\n",
       "      <td>3083078947</td>\n",
       "      <td>RT @ericswalwell: The #AmericanRescuePlan puts...</td>\n",
       "      <td>#</td>\n",
       "      <td>Thomas Albrecht üá∫üá∏‚òÆÔ∏è</td>\n",
       "      <td>TomAlb88</td>\n",
       "      <td>False</td>\n",
       "      <td>#</td>\n",
       "      <td>...</td>\n",
       "      <td>377609596</td>\n",
       "      <td>1369727803768201218</td>\n",
       "      <td>ericswalwell</td>\n",
       "      <td>377609596</td>\n",
       "      <td>ericswalwell</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369727803768201218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123693</th>\n",
       "      <td>2021-03-10 23:59:53+00:00</td>\n",
       "      <td>1369800204761899011</td>\n",
       "      <td>1</td>\n",
       "      <td>29801287</td>\n",
       "      <td>RT @TheDweck: Wow, vision boards work</td>\n",
       "      <td>#</td>\n",
       "      <td>Fauxnly Fans</td>\n",
       "      <td>thenickkontz</td>\n",
       "      <td>False</td>\n",
       "      <td>√úT: 43.508306,-96.779489</td>\n",
       "      <td>...</td>\n",
       "      <td>98247788</td>\n",
       "      <td>1369742802590990336</td>\n",
       "      <td>TheDweck</td>\n",
       "      <td>98247788</td>\n",
       "      <td>TheDweck</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369742802590990336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123694</th>\n",
       "      <td>2021-03-10 23:59:53+00:00</td>\n",
       "      <td>1369800205445521409</td>\n",
       "      <td>1</td>\n",
       "      <td>1095155084</td>\n",
       "      <td>just saw some lady on the news say she‚Äôs not g...</td>\n",
       "      <td>#</td>\n",
       "      <td>cristal‚ú®</td>\n",
       "      <td>cristal_guz</td>\n",
       "      <td>False</td>\n",
       "      <td>Raleigh |22|</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123695</th>\n",
       "      <td>2021-03-10 23:59:53+00:00</td>\n",
       "      <td>1369800205592363018</td>\n",
       "      <td>1</td>\n",
       "      <td>3538956135</td>\n",
       "      <td>RT @Doc_Wolverine: \"Gee Doc, why are you pisse...</td>\n",
       "      <td>#</td>\n",
       "      <td>Jackaxed</td>\n",
       "      <td>Jackaxed</td>\n",
       "      <td>False</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>898321581444911108</td>\n",
       "      <td>1369771222481985543</td>\n",
       "      <td>Doc_Wolverine</td>\n",
       "      <td>898321581444911108</td>\n",
       "      <td>Doc_Wolverine</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369771222481985543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5123696 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at             tweet_id  credible  \\\n",
       "0       2021-02-24 18:00:10+00:00  1364636249852502018         1   \n",
       "1       2021-02-24 18:00:18+00:00  1364636282664574978         1   \n",
       "2       2021-02-24 18:00:30+00:00  1364636333596008449         1   \n",
       "3       2021-02-24 18:03:16+00:00  1364637028948709377         1   \n",
       "4       2021-02-24 18:03:35+00:00  1364637110951583746         1   \n",
       "...                           ...                  ...       ...   \n",
       "5123691 2021-03-10 23:59:52+00:00  1369800203939745796         1   \n",
       "5123692 2021-03-10 23:59:52+00:00  1369800204094963712         1   \n",
       "5123693 2021-03-10 23:59:53+00:00  1369800204761899011         1   \n",
       "5123694 2021-03-10 23:59:53+00:00  1369800205445521409         1   \n",
       "5123695 2021-03-10 23:59:53+00:00  1369800205592363018         1   \n",
       "\n",
       "                   author_id  \\\n",
       "0                  107501328   \n",
       "1                   26761523   \n",
       "2        1234926105234034689   \n",
       "3        1329106574082641920   \n",
       "4        1363750425459970048   \n",
       "...                      ...   \n",
       "5123691            434360613   \n",
       "5123692           3083078947   \n",
       "5123693             29801287   \n",
       "5123694           1095155084   \n",
       "5123695           3538956135   \n",
       "\n",
       "                                                      text  \\\n",
       "0        RT @Maricopahealth: At one of our community po...   \n",
       "1        Ready for DAY 2 of State of the Valley? Join u...   \n",
       "2        RT @SteveStaeger: When #COVID19Colorado is ove...   \n",
       "3        #SD37: Starting next week, @OCHealth will star...   \n",
       "4        RT @jatinde45666597: Vaccination has been star...   \n",
       "...                                                    ...   \n",
       "5123691  RT @Philo: The boys of #SouthPark are at it ag...   \n",
       "5123692  RT @ericswalwell: The #AmericanRescuePlan puts...   \n",
       "5123693              RT @TheDweck: Wow, vision boards work   \n",
       "5123694  just saw some lady on the news say she‚Äôs not g...   \n",
       "5123695  RT @Doc_Wolverine: \"Gee Doc, why are you pisse...   \n",
       "\n",
       "                                  urls                          name  \\\n",
       "0                                    #                 2-1-1 Arizona   \n",
       "1        jointventure.org,twitter.com,              Joint Venture SV   \n",
       "2                                    #  Colorado Coronavirus Updates   \n",
       "3           bit.ly,www.ocregister.com,              Senator Dave Min   \n",
       "4                                    #                  Reena Sharma   \n",
       "...                                ...                           ...   \n",
       "5123691                              #                          ami_   \n",
       "5123692                              #          Thomas Albrecht üá∫üá∏‚òÆÔ∏è   \n",
       "5123693                              #                  Fauxnly Fans   \n",
       "5123694                              #                      cristal‚ú®   \n",
       "5123695                              #                      Jackaxed   \n",
       "\n",
       "                username  verified                  location  ...  \\\n",
       "0             211arizona     False                   Arizona  ...   \n",
       "1        JointVentureSVN     False              San Jose, CA  ...   \n",
       "2        COVIDinColorado     False          Denver, Colorado  ...   \n",
       "3             SenDaveMin      True                Irvine, CA  ...   \n",
       "4            write2reena     False     Auckland, New Zealand  ...   \n",
       "...                  ...       ...                       ...  ...   \n",
       "5123691       ami_tvdfan     False                         #  ...   \n",
       "5123692         TomAlb88     False                         #  ...   \n",
       "5123693     thenickkontz     False  √úT: 43.508306,-96.779489  ...   \n",
       "5123694      cristal_guz     False              Raleigh |22|  ...   \n",
       "5123695         Jackaxed     False             United States  ...   \n",
       "\n",
       "           retweet_author_id           retweet_id  retweeted_screen_name  \\\n",
       "0                   29816986  1364632754042802176         Maricopahealth   \n",
       "1                          #                    #                      #   \n",
       "2                  182037688  1364293582157307906           SteveStaeger   \n",
       "3                          #                    #                      #   \n",
       "4        1295748297529884673  1364087633538859008        jatinde45666597   \n",
       "...                      ...                  ...                    ...   \n",
       "5123691             81766872  1369799981763162113                philoTV   \n",
       "5123692            377609596  1369727803768201218           ericswalwell   \n",
       "5123693             98247788  1369742802590990336               TheDweck   \n",
       "5123694                    #                    #                      #   \n",
       "5123695   898321581444911108  1369771222481985543          Doc_Wolverine   \n",
       "\n",
       "            user_mentions_id  user_mentions_screen_name  in_reply_to_user_id  \\\n",
       "0                   29816986             Maricopahealth                    #   \n",
       "1                          #                          #                    #   \n",
       "2                  182037688               SteveStaeger                    #   \n",
       "3                   36069538                   ochealth                    #   \n",
       "4        1295748297529884673            jatinde45666597                    #   \n",
       "...                      ...                        ...                  ...   \n",
       "5123691             23827692              ComedyCentral                    #   \n",
       "5123692            377609596               ericswalwell                    #   \n",
       "5123693             98247788                   TheDweck                    #   \n",
       "5123694                    #                          #                    #   \n",
       "5123695   898321581444911108              Doc_Wolverine                    #   \n",
       "\n",
       "         in_reply_to_tweet_id in_reply_to_username reference_type  \\\n",
       "0                           #                    #      retweeted   \n",
       "1                           #                    #              #   \n",
       "2                           #                    #      retweeted   \n",
       "3                           #                    #              #   \n",
       "4                           #                    #      retweeted   \n",
       "...                       ...                  ...            ...   \n",
       "5123691                     #                    #      retweeted   \n",
       "5123692                     #                    #      retweeted   \n",
       "5123693                     #                    #      retweeted   \n",
       "5123694                     #                    #              #   \n",
       "5123695                     #                    #      retweeted   \n",
       "\n",
       "                reference_id  \n",
       "0        1364632754042802176  \n",
       "1                          #  \n",
       "2        1364293582157307906  \n",
       "3                          #  \n",
       "4        1364087633538859008  \n",
       "...                      ...  \n",
       "5123691  1369799981763162113  \n",
       "5123692  1369727803768201218  \n",
       "5123693  1369742802590990336  \n",
       "5123694                    #  \n",
       "5123695  1369771222481985543  \n",
       "\n",
       "[5123696 rows x 27 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['created_at', 'tweet_id', 'credible', 'author_id', 'text', 'urls',\n",
       "       'name', 'username', 'verified', 'location', 'followers_count',\n",
       "       'following_count', 'tweet_count', 'like_count', 'quote_count',\n",
       "       'reply_count', 'retweet_count', 'retweet_author_id', 'retweet_id',\n",
       "       'retweeted_screen_name', 'user_mentions_id',\n",
       "       'user_mentions_screen_name', 'in_reply_to_user_id',\n",
       "       'in_reply_to_tweet_id', 'in_reply_to_username', 'reference_type',\n",
       "       'reference_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_days.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REACTIONS\n",
    "\n",
    "There are 3 types of reactions:\n",
    "- replies ('replied_to')\n",
    "- quotes ('quoted')\n",
    "- retweets ('retweeted')\n",
    "\n",
    "All possible combinations of reactions types you may wish to take into account further down the line are specified in the full list below. \n",
    "\n",
    "The reaction_types list should be equal to one of the elements of the full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_types_full_list = [['quoted'], \n",
    "                            ['quoted', 'retweeted'], \n",
    "                            ['replied_to'], \n",
    "                            ['replied_to', 'quoted'], \n",
    "                            ['replied_to', 'quoted', 'retweeted'],\n",
    "                            ['replied_to', 'retweeted']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can choose what (combination of) reaction types you wish to be included in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_types = reaction_types_full_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['replied_to', 'quoted']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reaction_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_to_opinion_changes(reaction_types):\n",
    "    \"\"\"Function to create the path to the opinion changes JSON file, based on the reaction types we took into consideration.\n",
    "\n",
    "    Args:\n",
    "        reaction_types (list): list of reaction types\n",
    "\n",
    "    Returns:\n",
    "        str: path to the opinion changes file\n",
    "    \"\"\"    \n",
    "    type = \"_\".join(reaction_types)\n",
    "    path = opinion_changes_path + f\"/{type}_OC.json\"\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_reactions(merged_days, reaction_types):\n",
    "    \"\"\"Function to group reactions based on the reaction types list given as an input parameter, by the\n",
    "    'author_id' and 'reference_id' columns. This means that each group of reactions contains a (set of) reaction(s)\n",
    "    posted by the user identified by the 'author_id' and the source tweet identified by the 'reference_id'.\n",
    "\n",
    "    Args:\n",
    "        merged_days (pandas.core.frame.DataFrame): dataframe with all the data\n",
    "        reaction_types (list): list of reaction types we want to consider\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary where the key is a tuple of the form (author_id, reference_id)\n",
    "              and the value is a dataframe with all reactions corresponding to that combination\n",
    "    \"\"\"    \n",
    "    reactions = merged_days[merged_days['reference_type'].isin(reaction_types)]\n",
    "    multiple_reactions = reactions[reactions.duplicated(subset=['author_id', 'reference_id'], keep=False)]\n",
    "\n",
    "    # group the rows by the two columns\n",
    "    grouped_df = multiple_reactions.groupby(['author_id', 'reference_id'])\n",
    "    groups_of_reactions = grouped_df.groups\n",
    "\n",
    "    return groups_of_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_of_reactions = group_reactions(merged_days, reaction_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12221"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups_of_reactions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an instance of the PySentiStr class to which we set the path to the Java executable and the path to the language folder. After this, we are all set to use the SentiStrength library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath(path_to_sentistrength_jar)\n",
    "senti.setSentiStrengthLanguageFolderPath(path_to_sentistrength_language_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiments(rows_indexes, dataset, stop_words):\n",
    "    \"\"\"Function to compute the sentiment list for a set of rows in the dataset (given by dataset), taking\n",
    "    into account the given stop words.\n",
    "\n",
    "    Args:\n",
    "        rows_indexes (pandas.core.indexes.numeric.Int64Index): indexes of rows in the dataset that we want to compute the sentiment for\n",
    "        dataset (pandas.core.frame.DataFrame): dataframe containing the dataset\n",
    "        stop_words (set): set of stop words\n",
    "\n",
    "    Returns:\n",
    "        list: list of sentiment scores for each row identified by rows_indexes\n",
    "    \"\"\"    \n",
    "    texts = [ clean_text(dataset.loc[index, 'text'], stop_words) \n",
    "             if dataset.loc[index, 'reference_type'] != 'retweeted' else 'extremely fabulous'\n",
    "             for index in rows_indexes ]\n",
    "    \n",
    "    sentiments = senti.getSentiment(texts, score='scale')\n",
    "\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_change(rows_indexes, dataset, stop_words):\n",
    "    \"\"\"Function to detect whether an opinion change occured within a group of reactions (replies/quotes/retweets).\n",
    "\n",
    "    Args:\n",
    "        rows_indexes (pandas.core.indexes.numeric.Int64Index): list of indexes in the original dataframe (dataset)\n",
    "                                                               where we aim to detect an opinion change\n",
    "                                                               (e.g. Int64Index([1848965, 1850146, 1850687], dtype='int64'))\n",
    "        dataset (pandas.core.frame.DataFrame): dataframe containing the opinion changes\n",
    "        stop_words (list): list of stopwords\n",
    "\n",
    "    Returns:\n",
    "        bool: boolean value which confirms or denies the existence of an opinion change between the rows analyzed\n",
    "    \"\"\" \n",
    "    sentiments = compute_sentiments(rows_indexes, dataset, stop_words)\n",
    "    sentiments = np.array(sentiments)\n",
    "\n",
    "    positive = np.any(sentiments > 0)\n",
    "    negative = np.any(sentiments < 0)\n",
    "\n",
    "    return positive and negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATION OF TEST DATA - NECESSARY WHEN RUNNING TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_groups(full_dataset):\n",
    "    replies = full_dataset[full_dataset['reference_type'] == 'replied_to']\n",
    "    multiple_replies = replies[replies.duplicated(subset=['author_id', 'reference_id'], keep=False)].copy()\n",
    "    multiple_replies['reference_id'] = multiple_replies['reference_id'].astype(int)\n",
    "    test_multiple_replies = multiple_replies.head(2000).copy()\n",
    "\n",
    "    # group the rows by the two columns\n",
    "    grouped_df = test_multiple_replies.groupby(['author_id', 'reference_id'])\n",
    "    test_groups = grouped_df.groups\n",
    "\n",
    "    return test_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_groups = create_test_groups(merged_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = merged_days.head(500).copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE:\n",
    "\n",
    "It took more than 80 minutes when I ran the creation of the opinion_changes dictionary, in parallel, on the all types of reactions -  replied_to, quoted, retweeted ... I did not manage to run the sequential algorithm on all reactions due to hardware limitations (kernel crashed or execution stalled) ...\n",
    "\n",
    "It took almost 11 minutes when I ran the creation of the opinion_changes dictionary, in parallel, on the replies in the dataset.\n",
    "\n",
    "I saved the resulting dictionaries into JSON files, which can be found in the files/ directory of the project. These can be imported into dictionaries with ease (code can be found in the next parts of the notebook).\n",
    "\n",
    "HOWEVER, if you still wish to run the creation of the opinion_changes dictionary using the algorithms, uncomment the commented cells between the lines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEQUENTIAL COMPUTATION - OPINION CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counter = 0\n",
    "progress = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(groups_of_reactions):\n",
    "    \"\"\"Function that prints the progress of the computation of the opinion_changes dictionary,\n",
    "    as it takes a lot of time for large datasets.\n",
    "\n",
    "    Args:\n",
    "        groups_of_reactions (dict): dictionary of reactions grouped by some columns \n",
    "                                    (we expect the columns to be 'author_id' and 'reference_id')\n",
    "    \"\"\"    \n",
    "    global group_counter\n",
    "    global progress\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    if ipython is not None:\n",
    "        group_counter = ipython.user_ns['group_counter']\n",
    "        progress = ipython.user_ns['progress']\n",
    "\n",
    "    group_counter += 1\n",
    "    if ((group_counter / len(groups_of_reactions)) >= progress):\n",
    "        print(f\"Progress: {group_counter} / {len(groups_of_reactions)} groups of reactions processed.\")\n",
    "        progress += 0.001\n",
    "    if group_counter == len(groups_of_reactions):\n",
    "        print(\"All groups have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opinion_changes(groups_of_reactions, dataset, progress_printing, stop_words):\n",
    "    \"\"\"Function to create the data structure associated with the groups (pairs of user id - source tweet id)\n",
    "    where an opinion change occured, i.e. when, between their interactions (e.g. one's replies to the other's original post),\n",
    "    there have been both positive and negative opinions.\n",
    "\n",
    "    Args:\n",
    "        groups_of_reactions (dict): dictionary of reactions grouped by some columns\n",
    "                                    (we expect the columns to be 'author_id' and 'reference_id')\n",
    "        progress_printing (bool): boolean value indicating whether the user wishes to print the progress of the groups processed or not\n",
    "                                  (this can be useful to track when processing large datasets - they usually take a lot of time)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary where the keys represent the groups where opinion changes occured (as tuples) and the values are\n",
    "              lists of the sentiments associated to the interactions within each group\n",
    "    \"\"\"    \n",
    "    if progress_printing == True:\n",
    "        opinion_changes = {}\n",
    "        for group, rows_indexes in groups_of_reactions.items():\n",
    "            print_progress(groups_of_reactions)\n",
    "            if opinion_change(rows_indexes, dataset, stop_words) == True:\n",
    "                opinion_changes[group] = compute_sentiments(rows_indexes, dataset, stop_words)\n",
    "    else:\n",
    "        print(\"Gradual progress will not be printed.\")\n",
    "        print(\"If you wish to see it, change the value of the progress_printing input parameter to True.\")\n",
    "        opinion_changes = { group: compute_sentiments(rows_indexes, dataset, stop_words) for group, rows_indexes in groups_of_reactions.items() \n",
    "                        if opinion_change(rows_indexes, dataset, stop_words) == True }\n",
    "    \n",
    "    return opinion_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_printing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opinion_changes = create_opinion_changes(groups_of_reactions, merged_days, progress_printing, stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARALLEL COMPUTATION - OPINION CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentiments_for_group(rows_indexes):\n",
    "    \"\"\"Function to compute the sentiments of the tweets provided by the row indexes within the merged_days dataframe.\n",
    "    Returns a list of sentiments corresponding to each of the tweets or an empty list if there was no opinion change \n",
    "    within that group.\n",
    "\n",
    "    Args:\n",
    "        rows_indexes (pandas.core.indexes.numeric.Int64Index): list of indexes in the original dataframe (dataset)\n",
    "                                                               where we aim to detect an opinion change\n",
    "                                                               (e.g. Int64Index([1848965, 1850146, 1850687], dtype='int64'))\n",
    "    Returns:\n",
    "        list: list of sentiments for the rows or empty list if there was no opinion change within that group.\n",
    "    \"\"\"    \n",
    "    global merged_days\n",
    "    global stop_words\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    if ipython is not None:\n",
    "        merged_days = ipython.user_ns['merged_days']\n",
    "        stop_words = ipython.user_ns['stop_words']\n",
    "\n",
    "    processed_values = []\n",
    "    if opinion_change(rows_indexes, merged_days, stop_words):\n",
    "        processed_values = compute_sentiments(rows_indexes, merged_days, stop_words)\n",
    "\n",
    "    return processed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dict_chunk(input_dict):\n",
    "    \"\"\"Function to create an opinion_changes dictionary only for a chunk of data.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): chunk of data\n",
    "\n",
    "    Returns:\n",
    "        dict: opinion_changes dictionary for a chunk of data\n",
    "    \"\"\"    \n",
    "    # Process a chunk of the input dictionary\n",
    "    processed_dict = {}\n",
    "    counter = 0\n",
    "    progress = 0.0001\n",
    "    \n",
    "    for group, rows_indexes in input_dict.items():\n",
    "        processed_values = process_sentiments_for_group(rows_indexes)\n",
    "        if processed_values:  # only add non-empty lists to the dictionary\n",
    "            processed_dict[group] = processed_values\n",
    "\n",
    "        counter += 1\n",
    "        if ((counter / len(input_dict)) >= progress):\n",
    "            print(f\"{counter} / {len(input_dict)} entries processed...\\n\")\n",
    "            progress += 0.0001\n",
    "        if counter == len(input_dict):\n",
    "            print(f\"Thread has finished processing all {len(input_dict)} entries.\")\n",
    "\n",
    "\n",
    "    return processed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dict_in_parallel(input_dict, num_processes=None):\n",
    "    \"\"\"Function to process the input dictionary of reactions in parallel and merge the atomic results together into a single dictionary,\n",
    "    which will be the final opinion_changes dictionary.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): dictionary of reactions grouped by some columns\n",
    "                           (we expect the columns to be 'author_id' and 'reference_id')\n",
    "        num_processes (int): number of parallel(worker) threads/processes. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: the final opinion_changes dictionary, which contains all the pairs of 'author_id' and 'reference_id'\n",
    "              (and their respective rows in the original dataframe) in the whole dataset, where an opinion change occured\n",
    "    \"\"\"    \n",
    "    # Default to using all available CPU cores\n",
    "    if num_processes is None:\n",
    "        num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # Split the input dictionary into smaller chunks for parallel processing\n",
    "    chunk_size = len(input_dict) // num_processes\n",
    "    input_chunks = [dict(list(input_dict.items())[i:i + chunk_size]) for i in range(0, len(input_dict), chunk_size)]\n",
    "\n",
    "    # Process the input chunks in parallel using a pool of worker processes\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        processed_dicts = pool.map(process_dict_chunk, input_chunks)\n",
    "\n",
    "    # Merge the processed dictionaries from each input chunk\n",
    "    processed_dict = {}\n",
    "    for d in processed_dicts:\n",
    "        processed_dict.update(d)\n",
    "\n",
    "    return processed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opinion_changes_parallel = process_dict_in_parallel(groups_of_reactions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE DICTIONARY TO JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_opinion_changes_to_JSON(opinion_changes, reaction_types):\n",
    "    \"\"\"Function to save the dictionary of opinion changes to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        opinion_changes (dict): dictionary with opinion changes\n",
    "        reaction_types (list): list of reaction types\n",
    "    \"\"\"    \n",
    "    path = create_path_to_opinion_changes(reaction_types)\n",
    "\n",
    "    # create a new dictionary with string keys\n",
    "    opinion_changes_for_JSON_file = {str(key): value for key, value in opinion_changes.items() }\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(opinion_changes_for_JSON_file, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['replied_to', 'quoted']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reaction_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_opinion_changes_to_JSON(opinion_changes_parallel, reaction_types)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DICTIONARY FROM JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opinion_changes(path_to_opinion_changes):\n",
    "    \"\"\"Function that generates a dictionary based on a JSON file which contains the opinion changes within the reactions of the dataset.\n",
    "\n",
    "    Args:\n",
    "        path_to_opinion_changes (str): path to the JSON file associated with the opinion changes within the reactions\n",
    "                                               (e.g. /your/path/to/research-internship/files/replied_to_opinion_changes.json)\n",
    "\n",
    "    Returns:\n",
    "        dict: the original dictionary containing opinion changes from reactions\n",
    "    \"\"\"    \n",
    "    with open(path_to_opinion_changes) as f:\n",
    "        # Load the JSON data into a Python dictionary\n",
    "        opinion_changes_from_file = json.load(f)\n",
    "        # Create a new dictionary with tuple keys\n",
    "        original_opinion_changes = {}\n",
    "        for key in opinion_changes_from_file:\n",
    "            # Convert the string key to a tuple\n",
    "            new_key = eval(key)\n",
    "            # Add the key-value pair to the new dictionary\n",
    "            original_opinion_changes[new_key] = opinion_changes_from_file[key]\n",
    "            \n",
    "    return original_opinion_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_changes = load_opinion_changes(create_path_to_opinion_changes(reaction_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of opinion changes out of the interactions where one user reacted multiple times to a source tweet:\n",
      "13.1%.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of opinion changes out of the interactions where one user reacted multiple times to a source tweet:\")\n",
    "print(f\"{round(len(opinion_changes) / len(groups_of_reactions) * 100, 1)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biggest_opinion_change(opinion_changes):\n",
    "    \"\"\"Function that returns the group (pair of user id - source tweet id) which interacted more than once \n",
    "    in the context of a single source tweet, i.e. one user posted more than one reply to the same source tweet, \n",
    "    where the user who reacted had the most drastic opinion change,\n",
    "    based on the previously computed sentiments of the text.\n",
    "\n",
    "    Args:\n",
    "        opinion_changes (dict): dictionary with opinion changes\n",
    "\n",
    "    Returns:\n",
    "        tuple: pair of user id - source tweet id, where the biggest opinion change occured\n",
    "        str: type of change that occured, e.g. one user tends to agree with the source tweet after some time, \n",
    "             when initially he disagreed or vice-versa\n",
    "    \"\"\"    \n",
    "    change_type = 'negative'\n",
    "    biggest_change = 0\n",
    "    target_group = tuple()\n",
    "    for group, sentiments in opinion_changes.items():\n",
    "        change = max(biggest_change, max(sentiments) - min(sentiments))\n",
    "        if change > biggest_change:\n",
    "            biggest_change = change\n",
    "            target_group = group\n",
    "    \n",
    "    min_sentiment_index = opinion_changes[target_group].index(min(opinion_changes[target_group]))\n",
    "    max_sentiment_index = opinion_changes[target_group].index(max(opinion_changes[target_group]))\n",
    "    change_type = 'positive' if min_sentiment_index < max_sentiment_index else change_type\n",
    "\n",
    "    return target_group, change_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_group, change_type = biggest_opinion_change(opinion_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118788479, 1367613364923424769)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reactions_with_biggest_opinion_change(reactions, target_group):\n",
    "    \"\"\"Function that queries the reactions dataset and returns a list of the actual texts that the pair of users\n",
    "     (the author of the reaction and the author of the source tweet) posted.\n",
    "     The user id and source tweet id are passed on as input parameters (the target group).\n",
    "\n",
    "    Args:\n",
    "        replies (pandas Dataframe): the dataframe with the reactions\n",
    "        target_group (tuple): pair of user ids - source tweet id, whose posts had the biggest opinion change\n",
    "\n",
    "    Returns:\n",
    "        list: list of texts posted by the 2 users\n",
    "    \"\"\"    \n",
    "    condition1 = reactions['author_id'] == target_group[0]\n",
    "    condition2 = reactions['reference_id'] == target_group[1]\n",
    "\n",
    "    return reactions[condition1 & condition2].loc[:, 'text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions_biggest_change = reactions_with_biggest_opinion_change(merged_days, target_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In January, a troop of gorillas at the zoo‚Äôs Safari Park tested positive for the virus. The zoo vaccinated four orangutans and five bonobos with the experimental vaccine, which is not designed for use in humans. https://t.co/1ME1BqpKPi',\n",
       " 'Infection of apes is a major concern for zoos and conservationists. They easily fall prey to human respiratory infections, and common cold viruses have caused deadly outbreaks in chimpanzees in Africa. https://t.co/1ME1BqpKPi',\n",
       " 'Scientists are worrying not just about the danger the virus poses to great apes and other animals, but also about the potential for the virus to gain a foothold in a wild animal population that could become a permanent reservoir and emerge at a later date to reinfect humans. https://t.co/1ME1BqpKPi',\n",
       " 'Infections in farmed mink have produced the biggest scare so far. When Danish mink farms were devastated by the virus, which can kill mink just as it kills people, a mutated form of the virus emerged from the mink and reinfected humans. https://t.co/1ME1BqpKPi',\n",
       " 'Denmark ended up killing as many as 17 million mink ‚Äî effectively wiping out its mink farming industry. In the United States, thousands of mink have died, and one wild mink has tested positive for the virus. https://t.co/1ME1BqpKPi']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reactions_biggest_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biggest_opinion_change_type(opinion_changes, group):\n",
    "    \"\"\"Function to detect what type of opinion change occured in the case of a group (pair of user id - source tweet id) \n",
    "    which interacted.\n",
    "\n",
    "    Args:\n",
    "        opinion_changes (dict): dictionary with opinion changes\n",
    "        group (tuple): pair of user id - source tweet id that interacted through reactions \n",
    "                       and the respondent changed his/her viewpoint w.r.t. a source tweet\n",
    "\n",
    "    Returns:\n",
    "        str: either 'positive' (if the respondent now agrees after initially disagreeing) or 'negative'\n",
    "    \"\"\"    \n",
    "    min_sentiment_index = opinion_changes[group].index(min(opinion_changes[group]))\n",
    "    max_sentiment_index = opinion_changes[group].index(max(opinion_changes[group]))\n",
    "    \n",
    "    change_type = 'negative'\n",
    "    change_type = 'positive' if min_sentiment_index < max_sentiment_index else change_type\n",
    "\n",
    "    return change_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask indicating what type of opinion change each group has\n",
    "mask = {group: biggest_opinion_change_type(opinion_changes, group) for group in opinion_changes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_count_in_dict(dict, value_to_count):\n",
    "    \"\"\"Function to count the occurences of a certain value in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        dict (dict): dictionary where we need to count the occurences of a value\n",
    "        value_to_count (any): value to be counted\n",
    "\n",
    "    Returns:\n",
    "        int: number of occurences of value_to_count\n",
    "    \"\"\"    \n",
    "    # Create a reverse dictionary that maps values to their frequencies\n",
    "    reverse_dict = defaultdict(int)\n",
    "    for value in dict.values():\n",
    "        reverse_dict[value] += 1\n",
    "\n",
    "    # Count the occurrences of the specific value\n",
    "    count = reverse_dict.get(value_to_count, 0)\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive opinion changes out of:\n",
      "- the interactions where one user reacted multiple times to a source tweet and an opinion change was detected => 48.1%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of positive opinion changes out of:\")\n",
    "print(f\"- the interactions where one user reacted multiple times to a source tweet and an opinion change was detected => {round(value_count_in_dict(mask, 'positive') / len(mask) * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of negative opinion changes out of:\n",
      "- the interactions where one user reacted multiple times to a source tweet and an opinion change was detected => 51.9%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of negative opinion changes out of:\")\n",
    "print(f\"- the interactions where one user reacted multiple times to a source tweet and an opinion change was detected => {round(value_count_in_dict(mask, 'negative') / len(mask) * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_biggest_opinion_changes_deltas(path_to_opinion_changes):\n",
    "    opinion_changes = load_opinion_changes(path_to_opinion_changes)\n",
    "    deltas = { key: max(value) - min(value) for key, value in opinion_changes.items() }\n",
    "\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_labels = {\n",
    "    2: 'minimum',\n",
    "    3: 'slight',\n",
    "    4: 'considerable',\n",
    "    5: 'big',\n",
    "    6: 'very big',\n",
    "    7: 'huge',\n",
    "    8: 'maximum'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add value labels - adds the value of y\n",
    "def add_labels_y_value(x,y):\n",
    "    \"\"\"Function that takes the x and y-axis to be passed onto a plot function and generates labels,\n",
    "    such that on top of each y value, it is displayed centrally.\n",
    "\n",
    "    Args:\n",
    "        x (list): list of labels for x-axis of a plot\n",
    "        y (list): list of values for y-axis of a plot\n",
    "    \"\"\"    \n",
    "    for i in range(len(x)):\n",
    "        plt.text(x[i], y[i], y[i], ha = 'center', va = 'bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deltas_OC(reaction_types, deltas_labels):\n",
    "    opinion_changes_deltas = compute_biggest_opinion_changes_deltas(create_path_to_opinion_changes(reaction_types))\n",
    "    # Get the values from the dictionary\n",
    "    deltas = list(opinion_changes_deltas.values())\n",
    "\n",
    "    # Use Counter to count the occurrences of each value\n",
    "    deltas_count = Counter(deltas)\n",
    "    percentages = { deltas_labels[pair[0]]: round(pair[1] / sum(deltas_count.values()) * 100, 1) \n",
    "               for pair in sorted(deltas_count.most_common(), key=lambda x: x[0])}\n",
    "    \n",
    "    keys = list(percentages.keys())\n",
    "    values = list(percentages.values())\n",
    "\n",
    "    # Create a bar chart of the counts\n",
    "    plt.bar(keys, values, edgecolor='black')\n",
    "    # Add labels to the top of each bar\n",
    "    add_labels_y_value(keys, values)\n",
    "    plt.xlabel('Biggest difference in opinion')\n",
    "    plt.ylabel('Percentage of groups')\n",
    "\n",
    "    long_title = f'Distribution of the intensity of the biggest opinion changes for { \", \".join(reaction_types) }'\n",
    "    # Wrap the title onto multiple lines\n",
    "    wrapped_title = textwrap.fill(long_title, width=50)\n",
    "    plt.title(wrapped_title, loc=\"center\", pad=10)\n",
    "\n",
    "    types = \"_\".join(reaction_types)\n",
    "    path = covaxxy_deltas_OC_graphs_path + f\"/{types}_deltas_OC.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_deltas_OC(reaction_types_full_list, deltas_labels):\n",
    "    for reaction_types in reaction_types_full_list:\n",
    "        plot_deltas_OC(reaction_types, deltas_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_deltas_OC(reaction_types_full_list, deltas_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ADD VISUALIZATIONS FOR ALL INSIGHTS + IMPROVE PRINT MESSAGES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW COLUMN ADDITION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATION OF REPLIES_AND_QUOTES DATAFRAME, FOR WHICH WE WANT TO SEE IF THEY SUPPORT OR NOT THE SOURCE TWEET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replies_and_quotes(full_dataset):\n",
    "    condition_1 = full_dataset['reference_type'] == 'replied_to'\n",
    "    condition_2 = full_dataset['reference_type'] == 'quoted'\n",
    "\n",
    "    return full_dataset[condition_1 | condition_2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_and_quotes = create_replies_and_quotes(merged_days)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HELPER FUNCTIONS TO ADD A NEW COLUMN TO THE test_replies_and_quotes DATAFRAME IN PARALLEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_replies_and_quotes = replies_and_quotes.head(1000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "progress = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress():\n",
    "    global counter\n",
    "    global progress\n",
    "    global test_replies_and_quotes\n",
    "    ipython = get_ipython()\n",
    "    if ipython is not None:\n",
    "        counter = ipython.user_ns['counter']\n",
    "        progress = ipython.user_ns['progress']\n",
    "        test_replies_and_quotes = ipython.user_ns['test_replies_and_quotes']\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    if ((counter / len(test_replies_and_quotes)) >= progress):\n",
    "        print(f\"{counter} / {len(test_replies_and_quotes)} replies or quotes processed.\\n\")\n",
    "        progress += 0.001\n",
    "    if counter == len(test_replies_and_quotes):\n",
    "        print(\"New column inserted in the replies_and_quotes dataframe.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supports_source_tweet(text):\n",
    "    if not isinstance(text, str):\n",
    "        return '#'\n",
    "    \n",
    "    sentiment = senti.getSentiment(text, score='scale')[0]\n",
    "\n",
    "    # print_progress()\n",
    "    \n",
    "    return sentiment > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper function that applies supports_source-tweet to a chunk of data\n",
    "def apply_function_to_chunk(chunk):\n",
    "    return chunk.apply(supports_source_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_support_source_tweet_column_parallel(replies_and_quotes):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    num_chunks = multiprocessing.cpu_count()\n",
    "    chunks = np.array_split(replies_and_quotes['text'], num_chunks)\n",
    "\n",
    "    # Create a multiprocessing pool and apply the function to each chunk in parallel\n",
    "    with multiprocessing.Pool(processes=num_chunks) as pool:\n",
    "        results = pool.map(apply_function_to_chunk, chunks)\n",
    "\n",
    "    # Concatenate the results back into a single DataFrame\n",
    "    # replies_and_quotes['supports_source_tweet'] = pd.concat(results)\n",
    "    replies_and_quotes.insert(replies_and_quotes.columns.get_loc('text') + 1, 'supports_source_tweet', pd.concat(results))\n",
    "\n",
    "    return replies_and_quotes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARALLEL EXECUTION. Benchmark tests (my machine): 1000 recordings => 1m11.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_replies_and_quotes_parallel = add_support_source_tweet_column_parallel(test_replies_and_quotes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEQUENTIAL EXECUTION. Benchmark tests (my machine): 1000 recordings => 2m30.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_replies_and_quotes = replies_and_quotes.head(1000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_replies_and_quotes.insert(test_replies_and_quotes.columns.get_loc('text') + 1, 'supports_source_tweet', test_replies_and_quotes['text'].apply(supports_source_tweet))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK IF RESULTS ARE THE SAME FOR BOTH METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_replies_and_quotes_parallel.equals(test_replies_and_quotes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE TO MODIFY THE ORIGINAL DATAFRAME (WITH ALL REPLIES AND QUOTES), WITH AN ADDED COLUMN NAMED 'supports_source_tweet', USING PARALLEL COMPUTATION, AS WELL AS SAVE IT TO A .CSV FILE (UNCOMMENT CELLS TO RUN)\n",
    "\n",
    "NOTE: There are almost 1 million replies and quotes in the original dataframe, so the following statement is extremely time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replies_and_quotes = add_support_source_tweet_column_parallel(replies_and_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_replies_and_quotes = files_path + '/replies_and_quotes_modified.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the DataFrame to a CSV file\n",
    "# replies_and_quotes.to_csv(path_to_replies_and_quotes, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the distribution of the tweets per hour, I will parse the \"created_at\" column, extract the hour property and create a separate column in each dataframe. I will place it next to the \"created_at\" column in order to be easily verifiable. Data originates frmo the Twitter API, so it comes in a standard ISO 8601 format, which can be easily parsed using the parser module from the dateutil package.\n",
    "\n",
    "Note: the cell below runs for approximately 2m30' on my machine (~25-30 seconds for each file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, day in days.items():\n",
    "#     if 'hour' not in day.columns:\n",
    "#         day.insert(1, 'hour', day['created_at'].apply(lambda date: parser.parse(date).hour))\n",
    "#         print(f\"New 'hour' column inserted in the {key} dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, day in days.items():\n",
    "#     if 'hour' not in day.columns:\n",
    "#         hours = []\n",
    "#         for time in day.loc[:,\"created_at\"]:\n",
    "#             hour = parser.parse(time).hour\n",
    "#             hours.append(hour)\n",
    "#         day.insert(1, \"hour\", hours, True)\n",
    "#         print(key + \" - added 'hour' column\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final distribution is made up of the sum of all individual days' distributions. I save a figure in the graphs/ folder for each day, as well as an overall distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_distribution = pd.Series(0, index=days['1-3-2021'].loc[:,'hour'].sort_values(ascending=True).unique())\n",
    "# for key, day in days.items():\n",
    "#     hour_column_ascending = day.loc[:,\"hour\"].sort_values(ascending=True)\n",
    "#     distribution = hour_column_ascending.value_counts()[hour_column_ascending.unique()]\n",
    "#     final_distribution = final_distribution.add(distribution)\n",
    "#     axes = distribution.plot(kind='bar')\n",
    "#     figure_path = f\"{covaxxy_longitudinal_analysis_graphs}/{key}_distribution.png\"\n",
    "#     axes.figure.savefig(figure_path)\n",
    "#     plt.close()\n",
    "# axes = final_distribution.plot(kind='bar')\n",
    "# figure_path = f\"{covaxxy_longitudinal_analysis_graphs}/overall_distribution.png\"\n",
    "# axes.figure.savefig(figure_path)\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243101c165aceacaf115b49fc146265224cf91574f24df3021157e0d2dabdb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
